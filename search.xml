<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[集成学习常见模型]]></title>
    <url>%2F2018%2F07%2F24%2F%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[概念俗话说，“三个臭皮匠，顶个诸葛亮”，多个比较弱的人若能有一种方法集中利用他们的智慧，也可以达到比较好的效果，这就是集成学习的思想。 集成学习是指通过构建并结合多个学习器来完成学习任务的一种机器学习方法 其结构如下图所示： 根据个体学习器的特点，可以分为以下两类： 同类型（如全是神经网络）的个体学习器，又称基学习器，构成同质集成 不同类型的个体学习器，又称组件学习器，构成异质集成 一般而言，个体学习器是所谓的弱学习器，即泛化能力略优于随机猜测的学习器。使用弱学习器进行集成学习已经可以获得足够好的泛化性能。当然也可以使用比较强的学习器。 理想的个体学习器应该具有好而不同的特点，即： 好：有一定的准确性 不同：个体学习器之间应该具有差异 原理为什么使用多个弱学习器可以集成为一个强学习器呢？假设二分问题的最终结果是投票而来——即超过一半的弱学习器输出的结果为最终结果。即： $$H(x)=sign(\sum_{i=1}^{T} {h_i(x)})$$ 上式中H(x)为集成后的学习器，$h_i(x)$为第i个弱学习器，sign为理想激活函数。 假设每个弱学习器的错误率为p，则集成的错误率为：(f(x)为真实的标记函数) $$P(H(x)!=f(x))=\sum_{k=0}^{\frac T2} {C_T^k (1-p)^k p^{T-k}}$$ 可以证明上式在p&lt;0.5的条件下，随着T的增大，函数值指数级趋向于0，由此就得到了更强的学习器。 但是上式依赖于一个关键的条件： 基学习器的误差是相互独立的。 但是该条件在现实任务中是不成立的，因为这些基学习器都是围绕一个问题在同一个训练集上训练出来的。 所以，如何寻找好而不同的个体学习器，是集成学习要研究的重点。 常见模型一般而言，要取得好而不同的学习器，有以下两大类： 个体学习器之间存在着依赖关系，必须串行的生成个体学习器。典型方法例如AdsBoost。 个体学习器之间不存在强依赖关系，可以并行的生成。典型方法例如bagging、随机森林。 AdaBoostAdaBoost是Boosting一族的代表算法。 Boost的含义是增强，Boosting方法就是从弱学习算法出发，在前一个学习器的基础上反复学习，得到一系列弱分类器，然后组合弱分类器，得到一个强分类器。Boosting方法在学习过程中通过改变训练数据的权值分布，针对不同的数据分布调用弱学习算法得到一系列弱分类器。 根据定义可以知道： boost是迭代算法，是通过前一个弱学习器进行优化，改变训练数据分布，从而得到下一个弱学习器，最终将所有的学习器进行组合的算法。 boosting算法要求基学习器可以对特定的数据分布进行学习，一般以下两种方法: 采用重赋值法，根据样本分布对不同仰恩数据进行赋值，这要求基学习器可以对带权数据进行学习 采用重采样法：根据样本分布对原训练集进行重新采样以满足某个分布，进行下一轮次的学习。 AdaBoost最终的模型是对于基学习器的线性组合，即： $$H(x)=\sum_{t=1}^{T} {\alpha_t h_t(x)}$$ 而该算法的目标是最小化指数损失函数： $$l_{exp}(H|D) = E_{x-D}(e^{-f(x)H(x)})$$ 其中，D为样本分布，f(x)为真实标记函数，H(x)为集成输出函数，E代表期望。 可以证明，当损失函数最小时，分类错误率也将最小化。 利用求导技巧最小化上式损失函数，可以得到该算法的具体流程： 123456789101112131415161718192021# 算法输入：# 训练集M=&#123;(x1,y1),(x2,y2)......(xm,ym)&#125;# 基学习算法B，训练轮数T# 算法输出：# 集成之后的学习器def ensembleLearning(M,B,T): # 初始分布D1，即为均匀分布 D1(x)=1/m # 循环训练T轮 for t in range(0,T): # 训练得到学习器ht ht(x)=B(D,Dt) # 计算该学习器的错误率 pt = P(ht(x)!=f(x)) # 如果该学习器比随机性能还差，就停止算法 if pt &lt; 0.5: break # 更新第t轮学习器的权重和下一轮的数据分布 at = 0.5 ln((1-pt)/pt) Dt+1 = refresh(Dt)return H(x)=sum(at*ht) 可见，迭代过程主要更新两个指标，分别是第t轮的学习器权重$\alpha_t$和下一轮的数据分布$D_{t+1}$，他们的更新公式都是由最小化损失函数得来的: $$\alpha_t = \frac 12 ln(\frac {1-p_t} {p_t})$$ $$D_{t+1}(x)=D_t(x)e^{-f(x)\alpha_t h_t(x)}\frac {E_{x-D}[e^{-f(x)H_{t-1}(x)}]} {E_{x-D}[e^{-f(x)H_{t}(x)}]}$$ 其中$f(x)$真实标记函数，$D_t(x),D_{t+1}(x)$为第t、t+1轮的数据分布，$H_t(x)$为到第t轮为止的集成算法。 AdaBoost主要关注减低偏差，而非方差，故可以通过很弱的弱学习器构建较强的集成学习器。 baggingbagging属于无依赖型并行算法，它主要通过变换不同的数据集来得到不同的算法，然后通过简单投票法得到最后的结果。 如何变换不同数据集呢？可以采用之前介绍过的自助采样法： 自助法：假设有m个数据的数据集，每次有放回的从其中抽取一个样本，执行m次，最终大概有36.8%的数据未被抽取到，当做测试集，其余当做训练集。 ​ $$ \lim_{m \to +\infty } (1-\frac 1m)^m = \frac 1 e = 0.386(该式子实际是e的定义式) $$ 优点：在数据集较小时用处较大，划分出的多个数据集有利于集成学习 缺点：改变了原数据样本的分布，会引入偏差 可以将上述方法重复T次，得到T个不同的数据集，便可以训练得到T个不同的弱学习器。 然后这多个弱学习器，通过简单投票，票多的结果胜出的方法，集成得到最终的强学习器。 bagging算法主要是降低方差，故它在一些易受到扰动的基学习器上使用效果更佳明显——例如神经网络，非剪枝决策树等。 随机森林和bagging不同，随机森林采用另一种方法来达到好而不同的效果。随机森林是在决策树的基础上进行的。 在普通决策树的构建过程中，节点的划分是在剩余属性集合中找一个最优的属性集合——信息增益最大 在随机森林的构建过程中，现在属性集合中先随机选出一个容量为$k(推荐为log(d)，其中d为属性总数)$的属性子集，在从中找到最好的分类属性。 显然，k=d时，退化为普通决策树，k=1时，相当于随机划分。 随机森林在很多现实任务中都表现出了强大的泛化能力。 查看更多所有的文章都会在我的博客和我的知乎专栏同步进行更新，欢迎阅读 我的博客 知乎专栏]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素和半朴素贝叶斯]]></title>
    <url>%2F2018%2F07%2F22%2F%E6%9C%B4%E7%B4%A0%E5%92%8C%E5%8D%8A%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[引入贝叶斯决策论是概率框架下实施决策的基本方法，它基于概率和误判误差的最小化来进行判别，是一种分类问题的解法。 原理首先，将数据及其属性记为x，对于多分类任务，其可能的取值有N种，设为{$c_1,c_2,c_3…..c_N$}，统称为c。数据x的正确分类记为$c_x$。于是将算法的正确率记为： ​ $$P_{true}=\sum P(c_x|x)=P(c|x)​$$ 若算法可以对x进行正确分类，则记误判误差为0。若错误分类，误差记为1. 于是，算法的总误差R可以表示为： ​ $$R(c|x)=1-P(c|x)$$ 我们的目标是最小化$R(c|x)$，该目标可以转化为最大化P(c|x)。由逆概率公式可知： ​ $$P(c|x)=\frac {P(c)*P(x|c)} {P(x)}$$ 成功利用逆概率公式将未知概率转化为我们已知的概率。 由逆概率公式知，判定x为类别i(记为$c_i$)的概率为，: ​ $$P(c_i|x)=\frac {P(c_i)*P(x|c_i)} {P(x)}$$ 我们只需要求出x为任意类别的概率，若最大概率取在类别j处，便可以判定x为类别$c_j$。 由于在计算P($c_i$|x)时，需要计算的量有三个：P($c_i$),P(x|$c_i$),P(x)。 由于判别任务是对于所有数据而言的，所以P(x)的概率对于任何样本都是一样的，可以忽略不计。 P($c_i$)代表类别$c_i$在总体中所占的比例，可以利用$$p(c_i) = \frac{|D_i|}{|D|}$$计算得到，其中$D_i$代表类别为$c_i$的样本集合，D为总样本集合。 难点出现在计算P(x|$c_i$)上面，该如何计算出现某一个类别$c_i$中出现数据x的概率。 显然，由于x非常多，例如一个具有100个二值属性的x可以的取值有$2^{100}$种，在训练集中不可能取得x的所有取值的样本，所以我们必须要估计P(x|$c_i$)的值。 朴素贝叶斯由于x实际上是由n个属性组成的，记为{$x_1,x_2……x_n$}。朴素贝叶斯方法有一个重要假设： 对于已知的类别，假设其每个属性之间是独立的。 若假设成立，便可以有公式： ​ $$P(x|c_i) = \prod_{j=1}^{n} {P(x_j|c_i)}$$ 也就是说，有$c_i$类别中每个属性出现的概率即可求得整体的概率。 离散属性任务对于离散属性的任务而言，可以直接用频率代替概率，即： ​ $$P(x_j|c_i)=\frac {|D_{c_i,x_j}|} {|D_{c_i}|}$$ 由此整个预测所需要的条件已满足。 连续属性任务显然，不可能对连续属性任务套用公式$P(x_j|c_i)=\frac {|D_{c_i,x_j}|} {|D_{c_i}|}$,因为$x_j$可以取任何值，无法通过频率替代概率。 由此，我们应该先假定该属性具有某种确定的概率分布形式，在基于训练样本对概率分布的参数进行估计，这里的估计方法，常用最大似然法。 极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。 以下通过一个例子说明极大似然法如何估计参数（该图来自于北大信科罗定生老师）： 于是，我们可以利用极大似然法来估算出连续型属性所遵循的分布，就可以利用属性在某值的概率密度替代离散属性中的概率。 例如，我们假设某个连续属性$x_j$遵循正态分布$N(\mu,\sigma^2)$,对$c_i$类别的所有样本的$x_j$属性值进行最大似然估计，结果得到$\mu = 1, \sigma =1 $，于是该属性遵循分布: ​ $$x_j分布：N(1,1) = \frac 1 {\sqrt{2\pi}} e^{-(\frac {(x-1)^2} 2)}$$ 若某个样本x的属性$x_j=1$，便可以将该值带入上式算出概率密度。 由此整个预测所需要的条件已满足。 拉普拉斯修正在离散属性任务中，由于是直接将频率当成概率，所以若一个属性未出现，则计算出的概率始终0，为了避免这种错误，可以使用拉普拉斯修正使之平滑： ​ $$p(c_i) = \frac{|D_i|}{|D|} 变成 p(c_i) = \frac{|D_i|+1}{|D|+N}$$ ​ $$P(x_j|c_i)=\frac {|D_{c_i,x_j}|} {|D_{c_i}|}变成P(x_j|c_i)=\frac {|D_{c_i,x_j}|+1} {|D_{c_i}|+ N_j} $$ 其中N为类别个数，$N_j$为第j个属性的可能取值的数目。 拉普拉斯修正实际上是假设了属性值和类别的均匀分布，但是当样本数目较大时，该影响会被消除。 半朴素贝叶斯朴素贝叶斯的假设过于强势，在实际模型中有时并不满足，因为样本的很多属性可能存在关联关系，并不独立。例如一个病人有发烧，虚汗……等等多种症状，要判断他的病症类型，但是可能发烧会引起虚汗，这两个属性是相关的。 为了解决这个问题，科学家们又在朴素贝叶斯上引入了一些限制条件：允许一些属性之间拥有依赖关系，由此形成了半朴素贝叶斯。 根据不同模型允许存在的依赖关系的不同，有以下几种常用的半朴素贝叶斯模型。 上图中NB为朴素贝叶斯的依赖关系，可以看见该模型中不同属性$x_1,x_2,x_3……$相互独立。 SPODE模型SPODE模型假设所有的属性都依赖一个父类属性——称为“超父”，在上图SPODE中，通过一些模型选择方法来来确定超父属性。 确定超父属性后，概率公式变为了： ​ $$P(c_i|x) \implies ^{正比于} P(c_i)*\prod_{j=1}^{n} {P(x_j|c_i,x_父)}$$ 其中$x_父$即为超父属性。 AODE模型AODE模型同样假设所有的属性都依赖一个“超父”属性。但是AODE模型尝试将每个元素作为超父属性，建立SPODE模型，然后从中筛选较好的属性。将这些所有较好的属性集成起来作为最终的模型。 概率公式为： ​ $$P(c_i|x) \implies^{正比于} \sum_{所有被选中的x_j属性}P(c,x_j)*\prod_{j=1}^{n} {P(x_i|c_i,x_j)}$$ 那么哪些属性被选中呢？要选择有足够数据支撑的数据，即： ​ $$|D_{x_j}|&gt;=m$$ 其中m为确认的阈值。 TAN模型TAN模型也是假设每个属性只依赖一个属性，但是并不是统一的超父。相反，TAN将N个属性看成一个无向完全图，然后设定每条边的权重为两条边的相关性，计算公式如下： ​ $$I(x_i,x_j|y)=\sum_{c}P(x_i,x_j|c)*log{\frac {P(x_i,x_j|c)}{P(x_i|c)P(x_j|c)}}$$ 观察该式可知： 若$x_i,x_j$无关，即$P(x_i,x_j|c)=P(x_i|c)*P(x_j|c)$，则该式值为0 若$x_i,x_j$相关，则$P(x_i,x_j|c)&gt;P(x_i|c)*P(x_j|c)$，则该式为正值。 建立无向完全图之后，通过最大生成树算法，挑选根变量，并将边设置为有向。 建立依赖图之后，就可以和AODE中一样计算概率，只不过每个属性有自己独特的父类而已，其余皆相同。 以上三个模型中$P(x_j|c_i,x_父)，P(c_i,x_j)$也可以通过将频率看做概率的方法计算得到： ​ $$P(c_i,x_j)=\frac {D_{c_i,c_j}+1} {|D|+N*N_i}$$ ​ $$P(x_j|c_i,x_父)=\frac {|D_{c_i,x_j,x_父}|+1} {|D_{c_i,x_父}|+N_j}$$ 查看更多所有的文章都会在我的博客和我的知乎专栏同步进行更新，欢迎阅读 我的博客 知乎专栏]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型的选择、评估和优化-下]]></title>
    <url>%2F2018%2F07%2F21%2F%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%80%89%E6%8B%A9%E3%80%81%E8%AF%84%E4%BC%B0%E5%92%8C%E4%BC%98%E5%8C%96-%E4%B8%8B%2F</url>
    <content type="text"><![CDATA[模型评估上文叙述了当机器学习模型已经训练完成之后，我们该如何评估模型的好坏。 但是一般而言，机器学习模型的训练时间较长，在训练过程中，我们怎么样判断模型训练的状态和优劣呢？ 之前说过，训练过程中的最容易出现的问题就是过拟合和欠拟合，下面介绍判断拟合状态的方法。 之前介绍过方差、偏差的概念以及他们的意义，在训练过程中，我们会得到训练集和测试集的总误差，通过这两个参数，我们可以判断出算法的拟合状态。 当出现欠拟合时，偏差高，方差低 当出现过拟合时，方差高，偏差低 拟合状态与样本规模首先，无论是过拟合还是欠拟合，由于一个特定的函数对于更多的数据更难以拟合，但是对预测是有利的，所以训练集误差会增加，测试集误差会减少。 若出现欠拟合情形，增大数据量，训练集和测试集误差会趋于平缓，并且两者很接近，但是喂更多的数据基本无效。 若出现过拟合情形，增大数据量，训练集误差会持续增加，测试集误差会持续减少，并且两者相差较大。 拟合状态与正则化项在线性回归、支持向量机等算法中，都会加入正则化项来防止过拟合，因为正则化项可以使得参数尽可能小。 ​ $$J(h_w(x),y) = -\frac1m \lbrace \sum_{i=1}^{m} y^ilog(h_w(x^i)) +(1-y^i)log(1-h_w(x^i)) \rbrace + \frac \lambda {2m} \sum_{j=1}^{m} \theta_j^2$$ 例如该式中的$\lambda​$即为正则化项的参数，其值越大，越可以避免过拟合，但是可能会陷入欠拟合泥淖之中。 随着$\lambda$的增大，训练集误差会持续升高（因为拟合程度逐渐减小），而测试集误差会先下降后上升，期间有最低点。 如图，$\lambda$较小时，会发生过拟合，所以测试集误差会很大，$\lambda$较大时，会发生欠拟合，所以测试集误差也会很大。 拟合状态和模型复杂性显然，选择的特性越多，模型越复杂。例如一个普通的线性回归，我们就可以选择： ​ $$y_1=\theta_0 + \theta_1x$$ ​ $$y_2=\theta_0 + \theta_1x +\theta_2x^2$$ ​ $$y_2=\theta_0 + \theta_1x +\theta_2x^2+\theta_3x^3……$$ 模型越复杂，学习能力越强，故训练集误差越小。但是对于测试集误差，当它减少到一定程度时，模型可能因为过于复杂而出现过拟合现象，误差反而增大。 模型优化如果我们已经发现当前算法效果并不好，会试图对算法进行一些优化，例如：加更多的特征，增加数据集，增大正则化项等，下表列举了常见的措施和应对情况。 措施 应对情形 搜集更多的数据 过拟合，高方差 使用更少的特征 过拟合，高方差 增加额外的特征 欠拟合，高偏差 增加多项式特征 欠拟合，高偏差 减小λ的值 欠拟合，高偏差 增加λ的值 过拟合，高方差 查看更多所有的文章都会在我的博客和我的知乎专栏同步进行更新，欢迎阅读 我的博客 知乎专栏]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型的选择、评估和优化-上]]></title>
    <url>%2F2018%2F07%2F21%2F%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%80%89%E6%8B%A9%E3%80%81%E8%AF%84%E4%BC%B0%E5%92%8C%E4%BC%98%E5%8C%96-%E4%B8%8A%2F</url>
    <content type="text"><![CDATA[引入对于一个机器学习工程而言，我们可以选择的模型众多，就之前的章节，我们已经可以选择： 线性回归 logistics判别 决策树 神经网络 对于一个模型而言，我们也有很多模型参数需要人工选择，本章将对模型的评估选择和优化进行详细介绍。 概念介绍过拟合和欠拟合在机器学习中，我们期望通过训练集来得到在新样本上表现的很好的学习器，找出潜在样本的普遍规律，在训练过程中，可能会出现两种情形： 欠拟合：指对训练样本的一般性质尚未学好 。 过拟合：学习器把训练样本学得“太好”了的时候，可能把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降。 可以通过下图来辅助理解： 在机器学习中，我们尤其要预防过拟合的发生，但由于机器学习的问题常常是NP难甚至是NP完全的，而有效的算法必定是多项式时间内完成的，所以只要承认P=NP，就需要承认过拟合无法完全避免。 当然，P=NP是目前尚未证明或证伪的结论，我倾向于认为P!=NP。 偏差和方差对于测试样本x，其真实标记为y，数据集中的标记为$y_1$,机器学习算法输出的标记为$y_2$，算法输出的期望值为$\overline y_2$，则有以下定义： ​ $$方差：var(x)=E((y_2-\overline y_2)^2)$$ ​ $$偏差：bias^2(x) = E((y-\overline y_2)^2)$$ ​ $$噪声：\epsilon ^2 = E((y-y_1)^2)$$ ​ $$总误差：\sigma^2=E((y_1-y_2)^2)$$ 其中E()代表求期望，有数学推导可知： ​ $$\sigma^2=var(x)+bias^2(x)+\epsilon^2$$ 即总误差可以分解为方差、偏差与噪声之和。 从直观理解的角度上看： 偏差刻画了学习算法本身的拟合能力 方差刻画了数据集的变动导致学习性能的变化，也就是学习算法的稳定性 噪声表明了数据集标记本身的误差 查准率和查全率我们评估一个算法好坏的时候经常使用错误率，即算法输出与实际标记不一致的数据所占的比例。 然而有时候，我们会关心更加细致的数据，例如进行疾病检查，我们更关心检查出病症的病人占总病人数目的多少，或者是检测出病症的病人有多少真的有疾病，于是引入查准率和查全率： 首先，根据算法输出和实际标记，可以将数据分为四类： 真实情况 预测结果 正 反 正 TP FN 反 FP TN 表格中T和F分别代表True和False，P和N分别代表Positive和negative. 查准率P和查全率R定义如下： ​ $$P=\frac {TP}{TP+FP}$$ ​ $$R=\frac {TP} {TP+FN}$$ 查准率表示了被输出为正例的样本中真的是正例的比例 查全率表示了所有的正例中被算法识别出来的比例 模型选择一般而言，参数有两种，一种是模型中的参数，由算法进行自动的优化；另一种是模型本身自带的参数，又称超参数 在机器学习中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。 例如，我们再进行线性回归时，可以选择很多种形式的函数，例如: ​ $$y_1=\theta_0 + \theta_1x$$ ​ $$y_2=\theta_0 + \theta_1x +\theta_2x^2​$$ ​ $$y_2=\theta_0 + \theta_1x +\theta_2x^2+\theta_3x^3……$$ 也就是说，我们要选定到底哪个形式的模型效果会比较好，为了得到最好的模型，我们将数据集分为三个部分。 训练集（60%） 测试集（20%） 交叉验证集（20%） 其中训练集用于算法的训练，由此，我们可以得到很多个不同的模型，再使用交叉验证集分别测试每个模型的泛化能力，选择其中最优的模型。 最后，使用测试集来测试最优模型的泛化能力，为什么不直接使用上一步的交叉验证集呢？因为, 这个交叉验证集误差是我们通过对比选择出来的, 它在这个数据集上肯定是最优的, 相当于我们已经看到了这些数据, 用它来代表对未知数据的泛化能力显然不行。 在划分数据集的过程中，要保证数据集的划分尽可能保持数据分布的一致性，即使得它们独立同分布，数据集划分的方法一般而言有以下几种： 留出法：最基本的抽样方法，最好使用分层抽样保证数据分布一致性，也可以做多次划分，最后返回在每次划分上测试结果的平均值，可以避免偏差 优点：划分简单 缺点：可能产生偏差，若采取多次取样，则训练成本过高 交叉验证法：将数据集划分为k个大小相似的数据集，注意使用分层抽样。每次使用一个小数据集做测试集，其他k-1个做训练集，轮流进行k次，最后返回的是测试结果的平均值。 自助法：假设有m个数据的数据集，每次有放回的从其中抽取一个样本，执行m次，最终大概有36.8%的数据未被抽取到，当做测试集，其余当做训练集。 优点：在数据集较小时用处较大，划分出的多个数据集有利于集成学习。 缺点：改变了原数据样本的分布，会引入偏差 $$ \lim_{m \to +\infty } (1-\frac 1m)^m = \frac 1 e = 0.386(该式子实际是e的定义式) $$ 模型评估在前面，已经介绍过查准率和查全率，当想要评估一个模型的好坏时，便可以使用这两项指标。 实际上，除了一些很简单的任务外，查准率和查全率是无法两全的： 若要提高查准率，即提高判为正例的标准，那么必定会漏掉一些真的正例，降低查全率 若要提高查全率，即降低判为正例的标准，必定会混入假的正例，降低查准率。 对于一个算法而言，其查准率和查全率关系如图所示： 有了这两项指标，一般按照如下原则比较不同模型： 若A模型的P-R曲线完全包住B，则A模型优于B 根据P-R曲线包裹的面积，面积大的模型更优 根据模型的现实需要，选择P或R值更好的模型 根据$F_1$值，$F_1$值大的模型较为优秀 ​ $$F_1 = 2* \frac {1} {\frac 1P + \frac 1R}$$ 最好的方法是综合现实情形和$F_1$值，引入$F_{\beta}$，其中参数$\beta$是对于查全率的重视程度 ​ $$F_{\beta}=(1+\beta^2)\frac {1} {\frac 1P + \frac {\beta^2}{R}}$$ 查看更多所有的文章都会在我的博客和我的知乎专栏同步进行更新，欢迎阅读 我的博客 知乎专栏]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络初探]]></title>
    <url>%2F2018%2F07%2F19%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[神经网络初探在本文中，我将初步介绍神经网络有关的概念和推导，本文是后续深度学习的入门，仅对神经网络做初步理解，后续文章中会继续进行学习。 定义什么是神经网络呢？这里引用Kohonen的定义: 神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。 可以看出神经网络的几个重要特点：简单的基本单元、互连、模拟生物、具有交互反应。 为什么使用神经网络？既然已经有了线性回归、决策树等机器学习方法，为什么还要使用神经网络的方法呢？ 生物大脑内的神经网络即采用基本单元互联通信的模型，部分机器学习的专家相信，如果能在机器上模拟这种结构，也将部分的实现生物体的智能。 在对非线性的数据进行预测时，如果使用线性回归来进行拟合，将会使用大量幂次交叉项，例如对最高三次项函数，就有$x_1^2x_2,x_1x_2^2,x_1^3,x_2^3……$，而对于含有n个特征，最高m次的数据，其项数的复杂度为O($n^m$)，高昂的复杂度使得预测代价极为高昂。 由此，神经网络应运而生。 M-P神经元模型神经网络中的基本单元被称为M-P神经元模型，如图所示： 在生物中，一个神经元接受其他神经元传来的化学性物质，改变它的点位，如果达到该神经元的阈值，它会被激活，向其他神经元发送化学信号 在该M-P神经元中，该神经元结构其他神经元的输入信号$x_1,x_2……x_n$，由于每个神经元对它的作用大小不同，于是有不同的权重$w_1,w_2……w_n$，收到的信号总和为 ​ $$X=\sum_{i=1}^{n} {w_ix_i}$$ 该信号与阈值$\theta$比较，如果大于阈值，就激活，反之不激活，该比较有函数f完成，故输出信号为： ​ $$y=f(\sum_{i=1}^{n} {w_ix_i-\theta})$$ 激活函数 一个节点的激活函数定义了该节点在给定的输入或输入的集合下的输出，上文中的函数f即为激活函数。 理论上而言，激活函数应该具有这样的功能：大于阈值时激活，小于阈值时不激活。故理想的激活函数如下最左所示： 但是在实际运用中，由于需要该函数光滑可导，经常使用如图的sigmoid函数和ReLU函数。 感知机 感知器（英语：Perceptron）是Frank Rosenblatt在1957年就职于康奈尔航空实验室（Cornell Aeronautical Laboratory）时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。 其模型如下，和M-P神经元模型如出一辙： 利用单层感知机可以解决逻辑与、或、非问题,但是不能解决异或问题，因为感知机是一个线性分类器，而异或问题不可以被线性划分。 但是利用如下的双隐层感知机可以解决异或问题。 多层神经网络定义感知机是最简单的前馈神经网络，如果我们增加神经网络的层数和每层的数目，便会形成多层神经网络。 多层网络包含输入层、输出层、隐藏层，输入层用来接收外界输出，隐层和输出层对信号进行加工，最终结果由输出层神经元进行输出。 图中第一层即为输入层，最后一层为输出层，中间为三个隐层。 前向传播算法前向传播算法即神经网络进行识别匹配的算法，从输入信号得到输出信号，如下图，为具有两个隐层的神经网络。 先对图中符号进行定义，$x_1,x_2,x_3$是原始输入信号，$a_{ij}$是隐层和输出层得到的输入信号，$w_{ij}$为权重，$z_{ij}$是隐层得到的进一步传播的信号，$H(x)$是激活函数，$y_1,y_2,y_3$是输出结果。 首先我们得到了原始输入信号$x_1,x_2,x_3$，通过加权和进入隐层1: ​ $$a_{2j}=\sum_{i=1}^{3} {x_iw_{ji}}$$ 然后通过激活函数得到隐层1的输出 ​ $$z_{ij}=H(z_{ij}-\theta_{ij})$$ 重复此过程通过隐层2，得到$a_{41},a_{42},a_{43}$,即为输出层的输入信号。在前面已经说过，输出层也要对信号进行加工，所以再次通过H(x)进行处理，得到最终结果$y_1,y_2,y_3$。 输出类型对于二分类问题而言，只需要输出为0或1即可，所以只需要有一个输出单元，就像之前的感知机。 对于多分类问题而言，可以有k个分类，则需要有k个输出单元，每个输出单元输出0或1，共同组成一个k维向量，分别代表k个类别。如下四个向量便可以别代表1、2、3、4的类别。$$\begin{matrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{matrix}$$ 反向传播算法反向传播算法的数学推导较复杂，在这里不详细介绍，只大体介绍其思想： 定义神经网络输出值与实际值的误差，一般情况下有两种误差定义方法（$\theta$为神经网络中的权重）： 均方误差： ​ $$J(\theta)=\frac 12 \sum_{j=1}^{l}(y_j^k-y_j^{k2})^2$$ 对数误差 反向传播算法的目的是使得$J(\theta)$最小，可以采用梯度下降的方法，求得使之最小的$\theta$ 于是我们需要求得$J(\theta)$对于每一个参数$\theta _{ij}$的导数，然后利用梯度下降的公式对$\theta$进行迭代 但是我们无法直接求得导数，经过推导，可以发现每一层的误差导数和后一层的导数存在关系，所以可以先求得后一层的导数，然后依次向前推导，故称之为反向传播。 综合流程由上可知，一个多层神经网络的训练流程如下： 随机初始化 对于训练数据，利用前向传播计算出预测结果 利用预测结果和训练数据的标签计算误差 利用反向传播计算误差对于各个参数的导数 梯度下降，并重复此过程 避免局部最小的方法由于梯度下降可能导致神经网络陷入局部最小，而达不到全局最小值，所以在这里有以下集中方法缓解这个问题 以多组不同的初始值初始化神经网络进行训练，找出其中最好的结果作为最终参数 使用模拟退火技术，有一定概率接受更差的结果，且接受的概率随时间推移而降低 使用随机梯度下降法 深度学习实际上，科学家已经证明 只需一个包含足够多神经元的隐层，多层前馈网络就能以任意精度逼近任意复杂的连续函数。 但后来人们发现： 参数越多的模型复杂多越高，容量越大，这意味着它可以完成更复杂的学习任务。而增大网络深度有时比增多单隐层参数个数更有效。 随着云计算、大数据的到来，深度学习开始流行。 典型的深度学习就是很深层的神经网络，在后面的章节会进一步介绍。 查看更多所有的文章都会在我的博客和我的知乎专栏同步进行更新，欢迎阅读 我的博客 知乎专栏]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2018%2F07%2F18%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[定义决策树是一种常见的机器学习算法，它的思想十分朴素，类似于我们平时利用选择做决策的过程。 例如有人给我们介绍新的对象的时候，我们就要一个个特点去判断，于是这种判断的过程就可以画成一棵树，例如根据特点依次判断： 如上，决策的形式以树的形式进行示意和编码，就形成了决策树。 结构显然，决策树在逻辑上以树的形式存在，包含根节点、内部结点和叶节点。 根节点：包含数据集中的所有数据的集合 内部节点：每个内部节点为一个判断条件，并且包含数据集中满足从根节点到该节点所有条件的数据的集合。根据内部结点的判断条件测试结果，内部节点对应的数据的集合别分到两个或多个子节点中。 叶节点：叶节点为最终的类别，被包含在该叶节点的数据属于该类别。 简而言之，决策树是一个利用树的模型进行决策的多分类模型，简单有效，易于理解。 伪代码决策树算法的伪代码（参照了python语法）如下图所示： 123456789101112131415161718192021222324252627# D = &#123;(x1,y1)、(x2,y2)......(xm,yn)&#125; 是数据集# A = &#123;a1、a2、a3.&#125; 是划分节点的属性集# 节点node有两个主要属性：content代表该节点需要分类的训练集，type代表叶节点的决策类型def generateTree(D,A): newNode = 空 #生成新的节点 # 如果当前数据集都为一个种类，则设为一个叶节点并返回 if D 中数据皆属于类别 C: newNode.content = D newNode.type = C return # 如果已经没有属性了或者数据集在剩余属性中表现相同（属性无法区分） if A = 空集 or D中数据在A中取值相同: newNode.content = D newNode.type = D中最多的类 return #从A中选取最优的属性a a=selectBestPorperty(A) #为a的每一个取值生成一个节点，递归进行处理 for a的每一个取值 res[i]: 生成新的分支节点 node[i] D[i] = D中取值为res[i]的数据 node[i].content = D[i] if node[i].content == null: node[i].type = D中最多的类 else: generateTree(D[i],A - &#123;a&#125;) return 划分选择可以看到，在伪代码中，大部分步骤都是简单而明确的，而最重要的步骤在于从A中选取最优的属性a，可以说，属性选择的质量，决定了决策树的预测准确度。这很容易理解，例如我们看一个学生聪明与否可以看他的成绩，但是如果依靠他的身高预测他是否聪明，显然得不到好的结果。 一般的原则是，希望通过不断划分节点，使得一个分支节点包含的数据尽可能的属于同一个类别，即“纯度“越来越高。 这里列出三种常用的准则。 信息增益准则我们先对一个节点的纯度进行定义，我们将其称之为信息熵： ​ $$ Ent(D)= - \sum_{k=1}^{|\gamma|}p_klog(p_k) $$ 其中$p_k$代表当前节点D的数据中第k类样本所占的比例。 观察该信息熵的定义，有以下几个特点： 由于$p_k$都属于[0,1]，Ent(D)必定为正值，值越大说明纯度越低 Ent(D)在k=1，$p_1$=1时取值最小值0，在$k=|\gamma|$ $p_k=\frac 1 {|\gamma|}$时取值最大值$log_2^{|\gamma|}$ 信息熵是一个节点的固有性质，和该节点选取什么属性进行下一步的划分无关 在定义了信息熵之后，对信息增益进行定义，假设选取属性a有V个取值，${a^1 a^2 …… a^V}$，按照决策树的规则，D将被划分为V个不同的节点数据集，$D^v$代表其中第v个节点： ​ $$Gain(D,a)=Ent(D)-\sum_{v=1}^V \frac {|D^v|}{|D|}Ent(D^v)$$ 观察该式，有以下几点说明： 第一线Ent(D)是确定的，和选取的属性a无关，我们可以将之看为定值 $\frac {|D^v|}{|D|}$表示分支节点所占的比例大小，显然数据集越大的分支节点权重越高 分支节点整体纯度越大，则后一项越小，信息增益Gain变得越大，所以我们的目标是如何最大化信息增益 由此，我们得到了一种选择划分属性的方法，计算以每个属性进行划分子节点得到的信息增益，选择其中最大的作为选择的属性。 信息增益率准则信息增益原则对于每个分支节点，都会乘以其权重，也就是说，由于权重之和为1，所以分支节点分的越多，即每个节点数据越小，纯度可能越高。这样会导致信息熵准则偏爱那些取值数目较多的属性。 为了解决该问题，这里引入了信息增益率，定义如下： ​ $$Gain_ratio(D,a)=\frac {Gain(D,a)} {IV(a)}$$ ​ $$IV(a)=\sum_{v=1}^{V} {\frac{|D^v|}{|D|}log_2^{\frac{|D^v|}{|D|}}}$$ 相当于引入了修正项IV(a)，它是对于属性a的固有值。 需要注意的是，信息增益率原则可能对取值数目较少的属性更加偏爱,为了解决这个问题，可以先找出信息增益在平均值以上的属性，在从中选择信息增益率最高的。 基尼指数准则在CART决策树中，使用基尼指数来选择属性，首先定义数据集D的基尼值： ​ $$Gini(D)=\sum_{k=1}^{|\gamma|}{\sum_{k^1!=k}^{} {p_kp_{k^1}}}=1-\sum_{k=1}^{|\gamma|}{p_k^2}$$ 形象的说，基尼值代表了从D中随机选择两个样本，其类别不一致的概率。 有了基尼值后，可以在此基础上定义基尼指数： ​ $$Gini_index(D,a)=\sum_{v=1}^{V}{\frac{|D^v|}{|D|}Gini(D^v)}$$ 其中$D^v$的含义和之前相同，可以看出基尼指数越小，说明纯度越高，我们可以通过选择基尼指数小的属性来划分子节点。 剪枝剪枝是应该决策树过拟合的一种重要方法，主要分为以下两种： 预剪枝：该策略就是在对一个节点进行划分前进行估计，如果不能提升决策树泛化精度，就停止划分，将当前节点设置为叶节点。那么怎么测量泛化精度，就是留出一部分训练数据当做测试集，每次划分前比较划分前后的测试集预测精度。 优点：降低了过拟合风险，降低了训练所需的时间。 缺点：预剪枝是一种贪心操作，可能有些划分暂时无法提升精度，但是后续划分可以提升精度。故产生了欠拟合的风险。 后剪枝：该策略是首先正常建立一个决策树，然后对整个决策树进行剪枝。按照决策树的广度优先搜索的反序，依次对内部节点进行剪枝，如果将某以内部节点为根的子树换成一个叶节点，可以提高泛化性能，就进行剪枝。 优先：降低过拟合风险，降低欠拟合风险，决策树效果提升比预剪枝强 缺点：时间开销大得多 特殊值处理连续值处理在之前进行选择属性的时候，我们仅仅讨论了属性值为离散值的情况，例如身高分为“极高、高、较高、中等、较矮”五个选项，但是如果数据集中身高为连续值，例如140-210cm，我们该如何处理呢？ 这里可以采用二分的思想，将连续值化为离散值。由于我们的数据集是有限的，即使是连续值，属性a在数据集中也只出现了有限个确定的值，记为$(a_1,a_2,a_3……a_n)$，且$a_1&lt;a_2&lt;a_3……&lt;a_n$。 取n个值的中点，令 ​ $$t_1=\frac{a_1+a_2}2,t_2=\frac{a_2+a_3}2……t_{n-1}=\frac{a_{n-1}+a_n}2$$ 我们得到了n-1个中点，$(t_1，t_2……t_{n-1})$,任取一个值$t_i$可以将数据集D分为两个，$D^+$表示D中大于$t_i$的数据，$D^-$表示D中小于$t_i$的数据集合，这样，我们便可以同离散值一样进行处理了。 接下来的问题是，选取哪一个t呢？显然在信息增益准则下，应该选择使得信息增益最大的t: ​ $$Gain(D,a)=max_tGain(D,a,t)=max_tEnt(D)-\sum_{\lambda \in{+,-}}{\frac{|D^{\lambda}|}{|D|}Ent(D_t^{\lambda})}$$ 经过稍加改造的信息增益公式就可以选择最好的t来进行划分。 缺失值处理缺失值处理较为复杂，设计到较多的公式，在这里给出链接，读者可以参考阅读 缺失值处理详解 其主要思想是 在选择属性时，仅使用不缺失该属性的数据来计算信息增益，最后乘以一个代表缺失数据比例的比例系数 在对某个属性进行划分子节点时，对于不缺失该属性的数据正常划分，对于缺失该属性的数据，按不同的权重划分进行每个子节点 多变量决策树实际上大部分机器学习的分类算法，都是将一个具有n个属性的数据，看成一个在n维空间的一个点，分类的过程就是在n维空间或者更高维度空间中找到超平面，将这些点进行划分。 而普通的决策树算法有一个特点，由于它每个节点的划分条件都是单独的，明确的，所以决策树的决策边界是平行于空间的坐标轴的。如下图所示： 这对其拟合特性有一定的影响，当数据比较复杂时，需要较多的属性才能得到较好的划分，而多变量决策树就可以解决该问题。 在多变量决策树的学习过程中，不是为每个非叶结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器。 如下图所示： 多变量决策树较复杂，如果想进一步了解，可以阅读这个领域的论文。 查看更多所有的文章都会在我的博客和我的知乎专栏同步进行更新，欢迎阅读 我的博客 知乎专栏]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logistics判别与线性模型中的问题]]></title>
    <url>%2F2018%2F07%2F16%2Flogistics%E5%88%A4%E5%88%AB%E4%B8%8E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[之前说过，机器学习的两大任务是回归和分类，上章的线性回归模型适合进行回归分析，例如预测房价，但是当输出的结果为离散值时，线性回归模型就不适用了。我们的任务是：将回归分析中的实数值转化为离散值或者对于离散值的概率 logistic判别转换函数例如我们进行癌症判定，回归模型可以输出癌症几率a，并且 $a\in (0,1)$，而我们的任务是将几率转化为0、1两个结果（例如0表示无癌，1表示患癌）。如果我们使用前一章的线性回归模型，可以认为&gt;0.5的结果看成1，&lt;0.5的结果看成0，便可以得到下列的转换函数：$$result=0, y&lt;(=)0.5 \\result=1, y&gt;(=)0.5 \\$$这个公式看上去十分直观，但是有一些明显的缺点： 并非所有的连续值都可以均匀映射在[0,1]之间，例如人的身高在区间[120,250]之间，但是大部分人身高都取在该区间的中值，两端较少。 一些极端值可能会大大影响分类的效果，例如出现了一个身高0.7m的侏儒病患者，则映射区间变成了[70,250]-&gt;[0,1]，原来的中值180可能在这种情况下只能映射为0.3，使得分类效果变差。如下图所示： 为了解决以上这些问题，我们提出了一个特殊的转换函数，对数几率函数，又称sigmoid函数$$y=\frac {1} {1+e^{-x}}$$其图像如下： 可以很明显的看出，该函数将实数域映射成了[0,1]的区间，带入我们的线性回归方程，可得：$$y = \frac 1 {1+e^{-(\vec w^T \vec x+b)}}$$于是，无论线性回归取何值，我们都可以将其转化为[0,1]之间的值，经过变换可知：$$ln(\frac y {1-y}) = \vec w^T \vec x+b$$故在该函数中，$ln(\frac y {1-y})$ 代表了$\vec x$ 为正例的可行性大小，由于含有对数，所以称为对数几率，其中y为正例几率，1-y为反例几率。所以在算法中，最终得到的结果y便代表是正例的几率，它在[0,1]之间，一般而言，如果y大于0.5，我们将其视为正例，如果y小于0.5，我们将其视为反例。 更新策略我们将转换函数 $y = \frac 1 {1+e^{-(\vec w^T \vec x+b)}}$ 称为$h_w(x)$，这是原本的线性回归$ f(x) = w^Tx+b$ 和sigmoid函数$y=\frac {1} {1+e^{-x}}$结合得到的。由于这个函数并不是凸函数，直接带入我们之前的梯度下降策略是无效的，得不到优化的结果，所以要更换梯度下降策略。定义新的代价函数：$$Cost(h_w(x),y) = -log(h_w(x)) , y = 1 \\Cost(h_w(x),y) = -log(1-h_w(x)) , y = 0\\$$观察该式可知，无论y取0还是1，当$h_w(x)$与y差距越大，代价函数值越大，且趋于正无穷，代价函数取值范围为$[0,+ \infty]$。将上面的代价函数写成另一种形式，便于进行求导：$$Cost(h_w(x),y) = -ylog(h_w(x)) -(1-y)log(1-h_w(x))$$将所有的数据项代价累计起来，得到最终的代价函数形式：$$J(h_w(x),y) = -\frac1m \lbrace \sum_{i=1}^{m} y^ilog(h_w(x^i)) +(1-y^i)log(1-h_w(x^i)) \rbrace$$对于该函数使用梯度下降，分别对每一个$\vec w$的每一项进行求导和更新即可 正则化当我们利用线性回归拟合数据时，为了拟合较为复杂的数据，可能会引入较多的参数，例如：$$y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \theta_4 x_4 \\其中 x_1 = x , x_2 =x^2,x_3=x^3,x_4 = x^4$$我们可能会得到下面两种图像：在理想情况下，我们的算法应该得到左边的图像，而右边的图像显然有过拟合的倾向。 在统计学中，过拟合（英语：overfitting，或称过度拟合）现象是指在拟合一个统计模型时，使用过多参数。对比于可获取的数据总量来说，一个荒谬的模型只要足够复杂，是可以完美地适应数据。过拟合一般可以视为违反奥卡姆剃刀原则。当可选择的参数的自由度超过数据所包含信息内容时，这会导致最后（拟合后）模型使用任意的参数，这会减少或破坏模型一般化的能力更甚于适应数据。过拟合的可能性不只取决于参数个数和数据，也跟模型架构与数据的一致性有关。此外对比于数据中预期的噪声或错误数量，跟模型错误的数量也有关。 如果算法效果较好，即使我们在初始部分选择了很多个参数，如上文的$\theta_0 、 \theta_1 、 \theta_2 、\theta_3 、\theta_4$,理想的算法也应该尽量使$ \theta_3 、\theta_4$接近0，使得实际的得到的拟合曲线应该如左图所示，为解决该问题，我们引入了正则化项。 正则化线性回归为了解决过拟合的问题，我们应该引入一个参数项，使得在进行梯度下降的时候尽可能使得参数变小，这样可以使得很多额外的变量的系数接近于0。更新线性回归的代价函数：$$minS = \frac {1} {2n} (\sum_{i=1}^{n} {(f(x_i)-y_i)^2}+\lambda \sum_{j=1}^{m} \theta_j^2)$$其中, $\lambda \sum_{j=1}^{m} \theta_j^2$叫做正则化项(Regularization Term), ,λ叫做正则化参数(Regularization Parameter). λ的作用就是在”更好地拟合数据”和”防止过拟合”之间权衡.λ过大的话, 就会导致$\theta_1 \theta_1$…近似于0, 这时就变成了欠拟合(Underfit). 所以需要选择一个合适的λ。 正则化logistics判别和正则化线性回归相似，我们也在logistics判别中加入正则化项：$$J(h_w(x),y) = -\frac1m \lbrace \sum_{i=1}^{m} y^ilog(h_w(x^i)) +(1-y^i)log(1-h_w(x^i)) \rbrace + \frac \lambda {2m} \sum_{j=1}^{m} \theta_j^2$$接着进行梯度下降即可。 多分类问题logistics判别解决的是二分类问题，那么应该如何解决多分类问题呢？一般采用拆解法，来将多分类问题分解成多个二分类问题。一般而言有三种经典的拆分方法 一对一：假如某个分类中有N个类别，我们将这N个类别进行两两配对（两两配对后转化为二分类问题）。那么我们可以得到$\frac {N(N+1)} 2$个二分类器。之后在测试阶段，我们把新样本交给这个二分类器。于是我们可以得到个分类结果。把预测的最多的类别作为预测的结果。 一对其余：一对其余其实更加好理解，每次将一个类别作为正类，其余类别作为负类。此时共有（N个分类器）。在测试的时候若仅有一个分类器预测为正类，则对应的类别标记为最终的分类结果。若有多个分类器预测为正类，则选择概率最大的那个。 多对多：所谓多对多其实就是把多个类别作为正类，多个类别作为负类。该种方法比较复杂，这里推荐一个常用的方法：ECOC纠错码方法，这种方法简而言之，就是N个类别做多次划分形成M个分类器，用这M个分类器分别对N个类别进行识别，正例为1，负例为0，形成了M位的01编码，对于任何测试数据，形成其01编码，分别和N个类别的01编码进行比较，计算编码的距离，选取最近的类别作为测试数据的类别。 类别不均衡问题想象我们在做一个预测罕见病A的机器学习模型，但是该病十分罕见，我们一万个数据中只有8个病例，那么模型只需要将所有的数据都预测为无病，即可达到99.92%的超高预测成功率，但是显然这个模型不符合要求。那么对于这种数据集中类别不平衡的问题，该如何解决呢？目前主要有三种方法： 欠采样：去除一些数目过多的类别的数据，使得不同类别的数据数目接近。 优点：不需要重新收集数据，训练速度快 缺点：使用的数据集远小于原数据集，可能丢失重要信息 过采样：增加数目小的类别的数据，使得不同类别的数据数目接近。 优点：不丢失信息，数据集较大 缺点：若对数目少的数据进行重复采样会造成过拟合的问题，训练时间长 阈值移动：我们在之前logistics判别中说过，$ln(\frac y {1-y}) = \vec w^T \vec x+b$，我们通过$\frac y {1-y}$的值来判断正例负例，之前我们的判断依据是 $if \frac y {1-y} &gt; 1 $ ，阈值为1，我们设定新的阈值$$\frac {y_1} {1-y_1} = \frac y {1-y} * \frac {m_-} {m_+}$$其中$m_-,m_+$分别代表负例和正例的数目，如果两者接近，该阈值就为1。 查看更多所有的文章都会在我的博客和我的知乎专栏同步进行更新，欢迎阅读 我的博客 知乎专栏]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>logistics判别</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性回归和梯度下降]]></title>
    <url>%2F2018%2F07%2F05%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%2F</url>
    <content type="text"><![CDATA[在上一章我们说到，机器学习中主要的两个任务就是回归和分类。如果读者有高中数学基础，我们很容易回忆到我们高中学习过的一种回归方法——线性回归。我们将这种方法泛化，就可以得到机器学习中的一种常见模型——线性模型，线性模型是监督学习的一种。我们已经说过，我们要从数据集中训练出模型，每个数据可以视为（属性，标签）二元组。其中属性可以为属性向量。假设给定具有n个属性的属性向量的数据 $\vec x = (x_1,x_2,x_3 \dots x_n)$，我们利用属性的线性组合来进行预测，即$$f(x) = w_1x_1+w_2x_2+w_3x_3+ \dots + w_nx_n + b$$我们可以将其写成向量形式$$ f(x) = w^Tx+b$$其中$w=(w_1,w_2,w_3 \dots w_n)$，w和b就是该模型中我们要求的参数，确定w和b，该模型就得以确定。我们将这样的模型称为线性模型，不得不提的是，线性模型并不是只能进行线性分类，它具有很强的泛化能力，我们后面会提到。 属性转换在进行建模之前，我们要先对数据集进行处理，使得其适合进行建模。我们注意到，在线性模型中，属性值都是实数，那么会出现以下两种需要进行转化的情况 属性离散，但是有序关系（可以比较）。例如身材的过轻，正常，肥胖，过于肥胖，可以被编码为-1,0,1,2，从而转化为实数进行处理。 属性离散，但是无序关系（不可比较）。例如国籍的中国人，美国人，日本人。我们可以将取值有k种的值转化为k维向量，如上例，可以编码为 $(1,0,0),(0,1,0),(0,0,1)$。 单变量线性回归如果 $x = (x_1,x_2,x_3 \dots x_n)$中n= 1，此时x为一个实数，线性回归模型就退化为单变量线性回归。我们将模型记为$$f(x)=w x + b$$其中w,x,b都是实数,相信这个模型大家在高中都学习过。在这里我们有两种方法求解这个模型，分别是最小二乘法和梯度下降法。我们先定义符号，$x_i$ 代表第i个数据的属性值，$y_i$是第i个数据的标签值（即真值），f是我们学习到的模型，$f(x_i)$即我们对第i个数据的预测值。我们的目标是，求得适当的w和b，使得S最小，其中S是预测值和真值的差距平方和，亦称为代价函数，当然代价函数还有很多其他的形式。$$minS = \frac {1} {2n} \sum_{i=1}^{n} {(f(x_i)-y_i)^2}$$其中的$\frac1n$只是将代价函数值归一化的系数。 最小二乘法最小二乘法不是我们在这里要讨论的重点，但也是在很多地方会使用到的重要方法。最小二乘法使用参数估计，将S看做一个关于w和b的函数，分别对w和b求偏导数，使得偏导数为0，由微积分知识知道，在此次可以取得S的最小值。由这两个方程即可求得w和b的值。（此处省略过程）求得$$w = \frac {\sum_{i=1}^{n} {y_i(x_i-\overline x)}} {\sum_{i=1}^{n} {x_i^2} -\frac 1m(\sum_{i=1}^{n} {x_i})^2 }$$$$b = \overline y -b \overline x$$其中$\overline y，\overline x$分别是y和x的均值 梯度下降法我们刚刚利用了方程的方法求得了单变量线性回归的模型。但是对于几百万，上亿的数据，这种方法太慢了，这时，我们可以使用凸优化中最常见的方法之一——梯度下降法，来更加迅速的求得使得S最小的w和b的值。S可以看做w和b的函数 $S(w，b)$，这是一个双变量的函数，我们用matlab画出他的函数图像，可以看出这是一个明显的凸函数。梯度下降法的相当于我们下山的过程，每次我们要走一步下山，寻找最低的地方，那么最可靠的方法便是环顾四周，寻找能一步到达的最低点，持续该过程，最后得到的便是最低点。对于函数而言，便是求得该函数对所有参数（变量）的偏导，每次更新这些参数，直到到达最低点为止，注意这些参数必须在每一轮一起更新，而不是一个一个更新。过程如下：$$给w、b随机赋初值,一般可以都设为0$$$$w_{new} = w - a\frac {\partial S(w,b)}{\partial w}，b_{new} = b - a\frac {\partial S(w,b)}{\partial b}$$$$w = w_{new},b = b_{new}$$带入真正的表达式，即为$$w_0 = w_0 - a \frac1m \sum_{i=1}^{n} {(f(x_i)-y_i)},w_0是常数项$$$$w_j = w_j - a \frac1m \sum_{j=1}^{n} {(f(x_i)-y_i)x_i},j\in{1,2,3\cdots n}$$其中a为学习率，是一个实数。整个过程形象表示便是如下图所示，一步一步走，最后达到最低点。需要说明以下几点： a为学习率，学习率决定了学习的速度。 如果a过小，那么学习的时间就会很长，导致算法的低效，不如直接使用最小二乘法。 如果a过大，那么由于每一步更新过大，可能无法收敛到最低点。由于越偏离最低点函数的导数越大，如果a过大，某一次更新直接跨越了最低点，来到了比更新之前更高的地方。那么下一步更新步会更大，如此反复震荡，离最佳点越来越远。以上两种情况如下图所示 我们的算法不一定能达到最优解。如上图爬山模型可知，如果我们初始位置发生变化，那么可能会到达不同的极小值点。但是由于线性回归模型中的函数都是凸函数,所以利用梯度下降法，是可以找到全局最优解的，在这里不详细阐述。 多变量线性回归如果数据中属性是一个多维向量， $\vec x = (x_1,x_2,x_3 \dots x_n)$,那么该回归模型称为多变量线性回归。也就是一般意义上的线性回归模型。我们先定义符号，$\vec x_i$ 代表第i个数据的属性值，它是一个向量，$x_i^j$表示第i个数据的第j个属性，它是一个实数，$y_i$是第i个数据的标签值，也是实数。f是我们学习到的模型，$f(\vec x_i)$即我们对第i个数据的预测值。我们建立的模型为：$$f(\vec x_i) = \vec w^T \cdot \vec x + b$$我们的目标是，求得适当的 $\vec w$和b，使得S最小，其中S是预测值和真值的差距平方和，亦称为代价函数，当然代价函数还有很多其他的形式。$$minS = \frac {1} {2n} \sum_{i=1}^{n} {(f(\vec x_i)-y_i)^2}$$其中的$\frac1n$只是将代价函数值归一化的系数。 特征缩放由于$\vec x$具有很多维的特征，每一维的特征大小可能相差甚多，这样会大大影响学习的速度。假如房价范围0-10000000，房子大小范围1-200，那么这两个特征学习到的系数大小会差很多倍，而学习率必须按照最小的系数来进行设定，则大系数的收敛会非常慢。为了避免这种情况，我们使用了特征缩放将每个特征的值进行处理，使之在[-1,1]之间，当然，原本范围就于此在一个数量级的特征，也可以不进行处理。处理公式如下：$$x_i = \frac {x_i-\overline x} {x_{max}-x_{min}}$$或者$$x_i = \frac {x_i-\overline x} {\sigma}$$其中$\sigma$为数据标准差。 正规方程法对于多元线性回归而言，正规方程法是一种准确的方法，就像最小二乘法对于单变量线性回归一样。为了使形式更加简化，我们做以下符号设定$$\vec X = \left[ \begin{matrix} 1 &amp; x_1^1 &amp; x_1^2 &amp; x_1^3 \dots &amp; x_1^n \\ 1 &amp;x_2^1 &amp; x_2^2 &amp; x_2^3 \dots &amp; x_2^n \\ \cdots&amp; \cdots&amp; \cdots &amp; \cdots \\ 1 &amp;x_n^1 &amp; x_n^2 &amp; x_n^3 \dots &amp; x_n^n \\ \end{matrix} \right] \tag{3}$$由此，我们可以将S写成另一种形式，定义如下$$ S^1 = (\vec y -\vec X \cdot \vec w)^T(\vec y -\vec X \cdot \vec w) $$请注意，$S^1$和S的区别仅仅在于它没有$\frac 1n$的系数，而该系数是一个定值，故最小化的目标和过程是一样的，我们在此要将$S^1$最小化。同理，我们将$S^1$视为$\vec w$的函数，对于$\vec w$求导数，得到取得最小值时的$\vec w$的值，便是我们得到的结果，记为$\vec w^1$$$\vec w^1 =(\vec X^T \vec X)^{-1}\vec X^T\vec y$$该方法得到了为准确值，即在我们给定条件下的最优解，但是该方法有两个弊端： 需要计算$(\vec X^T \vec X)^{-1}$，相对于矩阵规模n而言，算法复杂度是O(n3), n非常大时, 计算非常慢，甚至根本无法完成。 可能出现矩阵不可逆的情况，在这里不进行数学上的分析，但是可以说明，以下两种情况容易导致矩阵不可逆。 我们使用了冗余的特征，例如我们选取的两个特征始终保持倍数关系，则这两个特征向量线性相关。此时应该去除冗余的向量。 我们使用了太多的特征(特征的数量超过了样本的数量).，也可以理解为样本的数量太少，对于这种情况我们可以删掉一些特征或者使用正则化（在下一篇文章中会讲到）。 梯度下降法此处的梯度下降法和之前一元线性回归的梯度下降法基本相同，无非是一元线性回归只有两个需要求的参数，而多元线性回归中有多个待求参数。其余的只需要将导数项换掉即可。最终得到的式子如下：$$w_0 = w_0 - a \frac1m \sum_{i=1}^{n} {(f(x_i)-y_i)},w_0是常数项$$$$w_j = w_j - a \frac1m \sum_{i=1}^{n} {(f(x_i)-y_i)x_i^j},j\in{1,2,3\cdots n}$$与正规方程法相比，梯度下降法当有大量特征时, 也能正常工作，仍可以在可接受的时间内完成。 泛化之前我们提到过，线性模型并不是只能进行线性分类，它具有很强的泛化能力，如果仅仅使用在此之前的单元和多元线性回归，我们只能得到多维空间的高维平面，为了进一步增强泛化能力，我们可以引入幂次项。比如我们原来有只有一个特征$x_1$，我们现在令$x_2=x_1^2$,就人为的引入了第二个特征，拥有更强的拟合能力。我们还可以引入两个特征的交叉项，使得线性模型更强大。例如，我们原本只有一个模型：$$y=w_1x_1+w_2x_2$$我们引入$x_3 = x_1^2,x_4=x_2^2,x_5=x_1x_2$，人为引入三个变量，我们的模型变为：$$y=w_1x_1+w_2x_2+w_3x_3+w_4x_4+w_5x_5$$也就是说，很多复杂的模型都可以转化为线性模型进行建模。但是，我们也要防范过拟合问题，过多的人为特征很容易导致过拟合，我们将在下一个章节详细讨论。 检验那么，我们写好算法进行运行之后，如何检验我们的算法是否正常运行呢？一个办法就是看他的S（总误差）随时间变化的图像。正常情况下，S应该随着算法的运行逐渐降低，降低的速度越来越小，但是如果算法错误，或者学习率不适宜，那么可能出现S反而增大或者抖动的现象，如下图所示： 总结线性模型以其简单和可解释性在众多模型中脱颖而出，至今仍是经常使用的回归算法之一，在机器学习中仍然具有重要应用，如趋势线，流行病学预测，金融经济等。 查看更多所有的文章都会在我的博客和我的知乎专栏同步进行更新，欢迎阅读 我的博客 知乎专栏]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习的概念、历史和未来]]></title>
    <url>%2F2018%2F06%2F29%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%A6%82%E5%BF%B5%E3%80%81%E5%8E%86%E5%8F%B2%E5%92%8C%E6%9C%AA%E6%9D%A5%2F</url>
    <content type="text"><![CDATA[相关概念提起机器学习，我们不得不给机器学习下一个准确的定义。在直观的层面，如果说计算机科学是研究关于算法的科学，那么机器学习就是研究关于“学习算法”的科学，或者说，不同于一般的显式编程，机器学习就是研究如何使得计算机在无法被显式编程的情况下进行学习的领域，需要注意的是，显式与否都是对于人类而言的——人类能否明确的搞清楚每个决策步骤，对于计算机而言，构成不同算法的代码与指令没有任何区别。更加精确的说，机器学习的定义如下： A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.一个（机器学习）的程序就是可以从经验数据E中对任务T进行学习的算法，它在任务T上的性能度量P会随着对于经验数据E的学习而变得更好 由于机器学习必然利用了某些经验，它们常常数据的形式存在，我们称之为数据集，其中的每个数据称为记录。例如我们通过一个人的性别、年龄和身高预测他是否患某种常见疾病，有以下数据： （性别：男；年龄：18；身高：174；是否得病：否）（性别：女；年龄：17；身高：164；是否得病：是）（性别：男；年龄：20；身高：181；是否得病：是）（性别：女；年龄：16；身高：161；是否得病：是） …… 这可以被称为一个数据集，其中每个人的数据称为记录。在记录中，关于该对象的描述型数据称为属性，由于属性往往有很多个——如上文的年龄，身高等，可以构成属性向量，这些向量张成的空间称为属性空间。而我们的算法需要预测那个量被称为标记（label）——在上文中便是“得病与否”。在有的数据集中存在标记，有的不存在。标记构成的空间称为标记空间，也称为输出空间。显然，由于我们只能得到整个总体数据的一部分——即训练样本，我们程序得到的模型却不能只适应于这个训练样本，它必须对整个总体数据都有比较好的预测效果。这就是说我们的模型必须具有泛化的能力。我们训练得到的模型称为一个假设，所有的模型一起构成了假设空间。显然，可能有多种假设空间和训练数据一致——就好像对于一个知识点很少的课堂学习，有不少人能得到很高的分数，但是对于整个总体数据，学习的不同模型显然效果差别很大——真正考验很多难的知识点的考试，考验把上述表面上的学霸分开。每个假设——也就是训练的模型，必然有其归纳偏好，也就是说，在训练集中没有见过的情况，或者两者皆可的情况，模型会选择哪种。归纳偏好是模型进行泛化的能力基础。那么，对于训练的得到多个不同模型，我们如何选择呢？常用的方法是奥卡姆剃刀： 奥卡姆剃刀：若有多个假设和观察一致，我们选择最简单的那个 奥卡姆剃刀基于一个朴素的哲学观念，即这个世界是简单的，可以理解的。 算法分类基于训练集是否拥有标记（label），我们可以把机器学习分为以下四类： 监督学习 无监督学习 半监督学习 强化学习 下面我们依次对他们进行解释。 监督学习：监督学习使用已知正确答案（label）的训练数据进行学习。就像一个学生得到了很多题目以及这些题目的答案，利用这些进行学习，最终希望可以做出更多的没有见过的题目。 无监督学习：无监督学习使用没有正确答案的数据进行学习。就像一个学生得到了很多练习题，尽管他不知道答案，但是他可以从中自行寻找规律。 半监督学习：该学习方法是以上两种的混合，在训练阶段结合了大量未标记的数据和少量标签数据。就好像一个学生得到了少量有答案的样例题目和大量无答案的练习题目进行学习。 强化学习：强化学习同样没有label，但是拥有回报函数来判断你是否更加接近目标。例如让学生搜寻某个正确答案，学生靠近正确答案，就进行奖励——比如给一个棒棒糖，如果更加偏离答案，就被杨永信电击一下，久而久之，学生会越来越靠近正确答案。 监督学习的任务亦可以分为两类： 分类：我们的目标应该是要对数据进行分类.，也就是说，我们预测的数据是离散的。例如：现在我们的数据是有关乳腺癌的医学数据, 它包含了肿瘤的大小以及该肿瘤是良性的还是恶性的. 我们的目标是给定一个肿瘤的大小来预测它是良性还是恶性. 回归：如果我们预测的数据是连续的，可以称为回归。例如： 我们想通过给定的一个房子的面积来预测这个房子在市场中的价格。 发展历程在历史上，人工智能的热潮和低谷已经度过了一轮又一轮，所以不得不提醒广大读者：一个技术必然是有其周期性，当前火热的深度学习完成不了强人工智能的历史使命，人工智能领域必然会再一次走向低谷，等待下一次技术迭代。那么机器学习和人工智能有什么关系呢？可以说，机器学习是人工智能发展到一定阶段的必然产物！从人们对于人工智能的认识来看，人工智能走过了以下几个阶段： 推理期：人们认为只要赋予机器以推理能力，机器就可以得到智能。典型代表：“逻辑理论家”程序，它证明了各种数学定理。 知识期：人们认为为了使机器有智能，不仅需要有逻辑推理能力，还需要有大量的知识。典型代表如专家系统。但人们发现需要输入的知识太多了，如果机器能自己学习知识，岂不是美滋滋？于是展开了关于机器学习的研究。 机器学习期：人们致力于研究如何让机器自己从样例中学习知识。 而对于机器学习而言，已经发展处以下一些流派，他们都在历史上繁荣一时，占据过一定的地位。 基于神经网络的“连接主义”：五十年代中后期闪亮登场，代表工作如”感知机“。 基于逻辑表达的“”符号主义“：六七十年代辉煌一时，代表工作如”结构学习系统“。 “统计学习”：九十年代中期开始兴起，代表技术是支持向量机和核方法。 目前，以深度学习为名的连接主义卷土而来，究其原因，不过是数据大了，计算能力强了——若数据样本过少，则容易“过拟合”，若没有强力计算设备，根本无法求解。所以，我在这里不得不再次提醒读者，目前深度学习并没有理论上的实质性突破，完成不了强人工智能的历史使命，人工智能领域必然会再一次走向低谷，等待下一次技术迭代，请不要把鸡蛋放在一个篮子里。 应用场景目前机器学习在各个领域发挥着重要领域，创造了无数的经济价值。以下举例说明 Google News搜集网上的新闻，并且根据新闻的主题将新闻分成许多簇, 然后将在同一个簇的新闻放在一起。 自动驾驶 结语机器学习拥有着广阔的应用场景和无限的前途，可以说，发展出能够取代人类的强人工智能，是整个计算机行业最大的目标。让我们一起交流学习，征服机器学习的星辰大海！ 查看更多所有的文章都会在我的博客和我的知乎专栏同步进行更新 我的博客 知乎专栏]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>概念</tag>
        <tag>历史</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习序言]]></title>
    <url>%2F2018%2F06%2F27%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%8F%E8%A8%80%2F</url>
    <content type="text"><![CDATA[专栏介绍机器学习专栏是我在2018年暑假开始全面入门机器学习的心得和总结，在这个为期接近三个月的暑假中，我会学习传统机器学习，深度学习，以及强化学习三类主要的机器学习方法。在本栏目中，我不会照葫芦画瓢的搬抄其他地方的资料，所有的文章均为原创，均为我的心得体会和总结。所以存在一定程度的简洁和省略。本专栏的目的是和想入门机器学习的朋友们一起交流，共同成长，也是对自己学习的一种监督和升华。本专栏所有文章都采用markdown书写，为保持风格一致，若有意为专栏投稿，请采取相同的格式。 作者介绍北大信科小码农一只，目前大二在读，爱好科幻、编程、游泳与思考，水平有限，梦想不小，愿与人交流，共同进步。个人知乎主页 学习资料 吴恩达机器学习课程-网易云课程 吴恩达深度学习-网易云课程 Reinforcement Learning 机器学习实战 Tensorflow：实战Google深度学习框架 TensorFlow实战 北大课程——游戏中的AI 西瓜书 更新事宜我会在学完并且理解一个章节后尽快进行更新，但不保障时间，更新时间尽量控制在一周两次左右。所有的文章都会在我的博客和我的知乎专栏同步进行更新 我的博客 知乎专栏 路漫漫其修远兮，吾将上下而求索，欢迎大家一起交流，共同学习]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>问候</tag>
        <tag>机器学习</tag>
        <tag>序言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你好，博客世界]]></title>
    <url>%2F2018%2F06%2F27%2F%E4%BD%A0%E5%A5%BD%EF%BC%8C%E5%8D%9A%E5%AE%A2%E4%B8%96%E7%95%8C%2F</url>
    <content type="text"><![CDATA[这是我的第一个博客，以后我会在这里分享知识、经验与看法~~~]]></content>
      <categories>
        <category>问候</category>
      </categories>
      <tags>
        <tag>测试</tag>
      </tags>
  </entry>
</search>
