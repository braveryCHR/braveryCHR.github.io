<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2018%2F07%2F18%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[定义决策树是一种常见的机器学习算法，它的思想十分朴素，类似于我们平时利用选择做决策的过程。 例如有人给我们介绍新的对象的时候，我们就要一个个特点去判断，于是这种判断的过程就可以画成一棵树，例如根据特点依次判断： 如上，决策的形式以树的形式进行示意和编码，就形成了决策树。 结构显然，决策树在逻辑上以树的形式存在，包含根节点、内部结点和叶节点。 根节点：包含数据集中的所有数据的集合 内部节点：每个内部节点为一个判断条件，并且包含数据集中满足从根节点到该节点所有条件的数据的集合。根据内部结点的判断条件测试结果，内部节点对应的数据的集合别分到两个或多个子节点中。 叶节点：叶节点为最终的类别，被包含在该叶节点的数据属于该类别。 简而言之，决策树是一个利用树的模型进行决策的多分类模型，简单有效，易于理解。 伪代码决策树算法的伪代码（参照了python语法）如下图所示： 123456789101112131415161718192021222324252627# D = &#123;(x1,y1)、(x2,y2)......(xm,yn)&#125; 是数据集# A = &#123;a1、a2、a3.&#125; 是划分节点的属性集# 节点node有两个主要属性：content代表该节点需要分类的训练集，type代表叶节点的决策类型def generateTree(D,A): newNode = 空 #生成新的节点 # 如果当前数据集都为一个种类，则设为一个叶节点并返回 if D 中数据皆属于类别 C: newNode.content = D newNode.type = C return # 如果已经没有属性了或者数据集在剩余属性中表现相同（属性无法区分） if A = 空集 or D中数据在A中取值相同: newNode.content = D newNode.type = D中最多的类 return #从A中选取最优的属性a a=selectBestPorperty(A) #为a的每一个取值生成一个节点，递归进行处理 for a的每一个取值 res[i]: 生成新的分支节点 node[i] D[i] = D中取值为res[i]的数据 node[i].content = D[i] if node[i].content == null: node[i].type = D中最多的类 else: generateTree(D[i],A - &#123;a&#125;) return 划分选择可以看到，在伪代码中，大部分步骤都是简单而明确的，而最重要的步骤在于从A中选取最优的属性a，可以说，属性选择的质量，决定了决策树的预测准确度。这很容易理解，例如我们看一个学生聪明与否可以看他的成绩，但是如果依靠他的身高预测他是否聪明，显然得不到好的结果。 一般的原则是，希望通过不断划分节点，使得一个分支节点包含的数据尽可能的属于同一个类别，即“纯度“越来越高。 这里列出三种常用的准则。 信息增益准则我们先对一个节点的纯度进行定义，我们将其称之为信息熵： ​ $$ Ent(D)= - \sum_{k=1}^{|\gamma|}p_klog(p_k) $$ 其中$p_k$代表当前节点D的数据中第k类样本所占的比例。 观察该信息熵的定义，有以下几个特点： 由于$p_k$都属于[0,1]，Ent(D)必定为正值，值越大说明纯度越低 Ent(D)在k=1，$p_1$=1时取值最小值0，在$k=|\gamma|$ $p_k=\frac 1 {|\gamma|}$时取值最大值$log_2^{|\gamma|}$ 信息熵是一个节点的固有性质，和该节点选取什么属性进行下一步的划分无关 在定义了信息熵之后，对信息增益进行定义，假设选取属性a有V个取值，${a^1 a^2 …… a^V}$，按照决策树的规则，D将被划分为V个不同的节点数据集，$D^v$代表其中第v个节点： ​ $$Gain(D,a)=Ent(D)-\sum_{v=1}^V \frac {|D^v|}{|D|}Ent(D^v)$$ 观察该式，有以下几点说明： 第一线Ent(D)是确定的，和选取的属性a无关，我们可以将之看为定值 $\frac {|D^v|}{|D|}$表示分支节点所占的比例大小，显然数据集越大的分支节点权重越高 分支节点整体纯度越大，则后一项越小，信息增益Gain变得越大，所以我们的目标是如何最大化信息增益 由此，我们得到了一种选择划分属性的方法，计算以每个属性进行划分子节点得到的信息增益，选择其中最大的作为选择的属性。 信息增益率准则信息增益原则对于每个分支节点，都会乘以其权重，也就是说，由于权重之和为1，所以分支节点分的越多，即每个节点数据越小，纯度可能越高。这样会导致信息熵准则偏爱那些取值数目较多的属性。 为了解决该问题，这里引入了信息增益率，定义如下： ​ $$Gain_ratio(D,a)=\frac {Gain(D,a)} {IV(a)}$$ ​ $$IV(a)=\sum_{v=1}^{V} {\frac{|D^v|}{|D|}log_2^{\frac{|D^v|}{|D|}}}$$ 相当于引入了修正项IV(a)，它是对于属性a的固有值。 需要注意的是，信息增益率原则可能对取值数目较少的属性更加偏爱,为了解决这个问题，可以先找出信息增益在平均值以上的属性，在从中选择信息增益率最高的。 基尼指数准则在CART决策树中，使用基尼指数来选择属性，首先定义数据集D的基尼值： ​ $$Gini(D)=\sum_{k=1}^{|\gamma|}{\sum_{k^1!=k}^{} {p_kp_{k^1}}}=1-\sum_{k=1}^{|\gamma|}{p_k^2}$$ 形象的说，基尼值代表了从D中随机选择两个样本，其类别不一致的概率。 有了基尼值后，可以在此基础上定义基尼指数： ​ $$Gini_index(D,a)=\sum_{v=1}^{V}{\frac{|D^v|}{|D|}Gini(D^v)}$$ 其中$D^v$的含义和之前相同，可以看出基尼指数越小，说明纯度越高，我们可以通过选择基尼指数小的属性来划分子节点。 剪枝剪枝是应该决策树过拟合的一种重要方法，主要分为以下两种： 预剪枝：该策略就是在对一个节点进行划分前进行估计，如果不能提升决策树泛化精度，就停止划分，将当前节点设置为叶节点。那么怎么测量泛化精度，就是留出一部分训练数据当做测试集，每次划分前比较划分前后的测试集预测精度。 优点：降低了过拟合风险，降低了训练所需的时间。 缺点：预剪枝是一种贪心操作，可能有些划分暂时无法提升精度，但是后续划分可以提升精度。故产生了欠拟合的风险。 后剪枝：该策略是首先正常建立一个决策树，然后对整个决策树进行剪枝。按照决策树的广度优先搜索的反序，依次对内部节点进行剪枝，如果将某以内部节点为根的子树换成一个叶节点，可以提高泛化性能，就进行剪枝。 优先：降低过拟合风险，降低欠拟合风险，决策树效果提升比预剪枝强 缺点：时间开销大得多 特殊值处理连续值处理在之前进行选择属性的时候，我们仅仅讨论了属性值为离散值的情况，例如身高分为“极高、高、较高、中等、较矮”五个选项，但是如果数据集中身高为连续值，例如140-210cm，我们该如何处理呢？ 这里可以采用二分的思想，将连续值化为离散值。由于我们的数据集是有限的，即使是连续值，属性a在数据集中也只出现了有限个确定的值，记为$(a_1,a_2,a_3……a_n)$，且$a_1&lt;a_2&lt;a_3……&lt;a_n$。 取n个值的中点，令 ​ $$t_1=\frac{a_1+a_2}2,t_2=\frac{a_2+a_3}2……t_{n-1}=\frac{a_{n-1}+a_n}2$$ 我们得到了n-1个中点，$(t_1，t_2……t_{n-1})$,任取一个值$t_i$可以将数据集D分为两个，$D^+$表示D中大于$t_i$的数据，$D^-$表示D中小于$t_i$的数据集合，这样，我们便可以同离散值一样进行处理了。 接下来的问题是，选取哪一个t呢？显然在信息增益准则下，应该选择使得信息增益最大的t: ​ $$Gain(D,a)=max_tGain(D,a,t)=max_tEnt(D)-\sum_{\lambda \in{+,-}}{\frac{|D^{\lambda}|}{|D|}Ent(D_t^{\lambda})}$$ 经过稍加改造的信息增益公式就可以选择最好的t来进行划分。 缺失值处理缺失值处理较为复杂，设计到较多的公式，在这里给出链接，读者可以参考阅读 缺失值处理详解 其主要思想是 在选择属性时，仅使用不缺失该属性的数据来计算信息增益，最后乘以一个代表缺失数据比例的比例系数 在对某个属性进行划分子节点时，对于不缺失该属性的数据正常划分，对于缺失该属性的数据，按不同的权重划分进行每个子节点 多变量决策树实际上大部分机器学习的分类算法，都是将一个具有n个属性的数据，看成一个在n维空间的一个点，分类的过程就是在n维空间或者更高维度空间中找到超平面，将这些点进行划分。 而普通的决策树算法有一个特点，由于它每个节点的划分条件都是单独的，明确的，所以决策树的决策边界是平行于空间的坐标轴的。如下图所示： 这对其拟合特性有一定的影响，当数据比较复杂时，需要较多的属性才能得到较好的划分，而多变量决策树就可以解决该问题。 在多变量决策树的学习过程中，不是为每个非叶结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器。 如下图所示： 多变量决策树较复杂，如果想进一步了解，可以阅读这个领域的论文。 查看更多所有的文章都会在我的博客和我的知乎专栏同步进行更新，欢迎阅读 我的博客 知乎专栏]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logistics判别与线性模型中的问题]]></title>
    <url>%2F2018%2F07%2F16%2Flogistics%E5%88%A4%E5%88%AB%E4%B8%8E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[之前说过，机器学习的两大任务是回归和分类，上章的线性回归模型适合进行回归分析，例如预测房价，但是当输出的结果为离散值时，线性回归模型就不适用了。我们的任务是：将回归分析中的实数值转化为离散值或者对于离散值的概率 logistic判别转换函数例如我们进行癌症判定，回归模型可以输出癌症几率a，并且 $a\in (0,1)$，而我们的任务是将几率转化为0、1两个结果（例如0表示无癌，1表示患癌）。如果我们使用前一章的线性回归模型，可以认为&gt;0.5的结果看成1，&lt;0.5的结果看成0，便可以得到下列的转换函数：$$result=0, y&lt;(=)0.5 \\result=1, y&gt;(=)0.5 \\$$这个公式看上去十分直观，但是有一些明显的缺点： 并非所有的连续值都可以均匀映射在[0,1]之间，例如人的身高在区间[120,250]之间，但是大部分人身高都取在该区间的中值，两端较少。 一些极端值可能会大大影响分类的效果，例如出现了一个身高0.7m的侏儒病患者，则映射区间变成了[70,250]-&gt;[0,1]，原来的中值180可能在这种情况下只能映射为0.3，使得分类效果变差。如下图所示： 为了解决以上这些问题，我们提出了一个特殊的转换函数，对数几率函数，又称sigmoid函数$$y=\frac {1} {1+e^{-x}}$$其图像如下： 可以很明显的看出，该函数将实数域映射成了[0,1]的区间，带入我们的线性回归方程，可得：$$y = \frac 1 {1+e^{-(\vec w^T \vec x+b)}}$$于是，无论线性回归取何值，我们都可以将其转化为[0,1]之间的值，经过变换可知：$$ln(\frac y {1-y}) = \vec w^T \vec x+b$$故在该函数中，$ln(\frac y {1-y})$ 代表了$\vec x$ 为正例的可行性大小，由于含有对数，所以称为对数几率，其中y为正例几率，1-y为反例几率。所以在算法中，最终得到的结果y便代表是正例的几率，它在[0,1]之间，一般而言，如果y大于0.5，我们将其视为正例，如果y小于0.5，我们将其视为反例。 更新策略我们将转换函数 $y = \frac 1 {1+e^{-(\vec w^T \vec x+b)}}$ 称为$h_w(x)$，这是原本的线性回归$ f(x) = w^Tx+b$ 和sigmoid函数$y=\frac {1} {1+e^{-x}}$结合得到的。由于这个函数并不是凸函数，直接带入我们之前的梯度下降策略是无效的，得不到优化的结果，所以要更换梯度下降策略。定义新的代价函数：$$Cost(h_w(x),y) = -log(h_w(x)) , y = 1 \\Cost(h_w(x),y) = -log(1-h_w(x)) , y = 0\\$$观察该式可知，无论y取0还是1，当$h_w(x)$与y差距越大，代价函数值越大，且趋于正无穷，代价函数取值范围为$[0,+ \infty]$。将上面的代价函数写成另一种形式，便于进行求导：$$Cost(h_w(x),y) = -ylog(h_w(x)) -(1-y)log(1-h_w(x))$$将所有的数据项代价累计起来，得到最终的代价函数形式：$$J(h_w(x),y) = -\frac1m \lbrace \sum_{i=1}^{m} y^ilog(h_w(x^i)) +(1-y^i)log(1-h_w(x^i)) \rbrace$$对于该函数使用梯度下降，分别对每一个$\vec w$的每一项进行求导和更新即可 正则化当我们利用线性回归拟合数据时，为了拟合较为复杂的数据，可能会引入较多的参数，例如：$$y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \theta_4 x_4 \\其中 x_1 = x , x_2 =x^2,x_3=x^3,x_4 = x^4$$我们可能会得到下面两种图像：在理想情况下，我们的算法应该得到左边的图像，而右边的图像显然有过拟合的倾向。 在统计学中，过拟合（英语：overfitting，或称过度拟合）现象是指在拟合一个统计模型时，使用过多参数。对比于可获取的数据总量来说，一个荒谬的模型只要足够复杂，是可以完美地适应数据。过拟合一般可以视为违反奥卡姆剃刀原则。当可选择的参数的自由度超过数据所包含信息内容时，这会导致最后（拟合后）模型使用任意的参数，这会减少或破坏模型一般化的能力更甚于适应数据。过拟合的可能性不只取决于参数个数和数据，也跟模型架构与数据的一致性有关。此外对比于数据中预期的噪声或错误数量，跟模型错误的数量也有关。 如果算法效果较好，即使我们在初始部分选择了很多个参数，如上文的$\theta_0 、 \theta_1 、 \theta_2 、\theta_3 、\theta_4$,理想的算法也应该尽量使$ \theta_3 、\theta_4$接近0，使得实际的得到的拟合曲线应该如左图所示，为解决该问题，我们引入了正则化项。 正则化线性回归为了解决过拟合的问题，我们应该引入一个参数项，使得在进行梯度下降的时候尽可能使得参数变小，这样可以使得很多额外的变量的系数接近于0。更新线性回归的代价函数：$$minS = \frac {1} {2n} (\sum_{i=1}^{n} {(f(x_i)-y_i)^2}+\lambda \sum_{j=1}^{m} \theta_j^2)$$其中, $\lambda \sum_{j=1}^{m} \theta_j^2$叫做正则化项(Regularization Term), ,λ叫做正则化参数(Regularization Parameter). λ的作用就是在”更好地拟合数据”和”防止过拟合”之间权衡.λ过大的话, 就会导致$\theta_1 \theta_1$…近似于0, 这时就变成了欠拟合(Underfit). 所以需要选择一个合适的λ。 正则化logistics判别和正则化线性回归相似，我们也在logistics判别中加入正则化项：$$J(h_w(x),y) = -\frac1m \lbrace \sum_{i=1}^{m} y^ilog(h_w(x^i)) +(1-y^i)log(1-h_w(x^i)) \rbrace + \frac \lambda {2m} \sum_{j=1}^{m} \theta_j^2$$接着进行梯度下降即可。 多分类问题logistics判别解决的是二分类问题，那么应该如何解决多分类问题呢？一般采用拆解法，来将多分类问题分解成多个二分类问题。一般而言有三种经典的拆分方法 一对一：假如某个分类中有N个类别，我们将这N个类别进行两两配对（两两配对后转化为二分类问题）。那么我们可以得到$\frac {N(N+1)} 2$个二分类器。之后在测试阶段，我们把新样本交给这个二分类器。于是我们可以得到个分类结果。把预测的最多的类别作为预测的结果。 一对其余：一对其余其实更加好理解，每次将一个类别作为正类，其余类别作为负类。此时共有（N个分类器）。在测试的时候若仅有一个分类器预测为正类，则对应的类别标记为最终的分类结果。若有多个分类器预测为正类，则选择概率最大的那个。 多对多：所谓多对多其实就是把多个类别作为正类，多个类别作为负类。该种方法比较复杂，这里推荐一个常用的方法：ECOC纠错码方法，这种方法简而言之，就是N个类别做多次划分形成M个分类器，用这M个分类器分别对N个类别进行识别，正例为1，负例为0，形成了M位的01编码，对于任何测试数据，形成其01编码，分别和N个类别的01编码进行比较，计算编码的距离，选取最近的类别作为测试数据的类别。 类别不均衡问题想象我们在做一个预测罕见病A的机器学习模型，但是该病十分罕见，我们一万个数据中只有8个病例，那么模型只需要将所有的数据都预测为无病，即可达到99.92%的超高预测成功率，但是显然这个模型不符合要求。那么对于这种数据集中类别不平衡的问题，该如何解决呢？目前主要有三种方法： 欠采样：去除一些数目过多的类别的数据，使得不同类别的数据数目接近。 优点：不需要重新收集数据，训练速度快 缺点：使用的数据集远小于原数据集，可能丢失重要信息 过采样：增加数目小的类别的数据，使得不同类别的数据数目接近。 优点：不丢失信息，数据集较大 缺点：若对数目少的数据进行重复采样会造成过拟合的问题，训练时间长 阈值移动：我们在之前logistics判别中说过，$ln(\frac y {1-y}) = \vec w^T \vec x+b$，我们通过$\frac y {1-y}$的值来判断正例负例，之前我们的判断依据是 $if \frac y {1-y} &gt; 1 $ ，阈值为1，我们设定新的阈值$$\frac {y_1} {1-y_1} = \frac y {1-y} * \frac {m_-} {m_+}$$其中$m_-,m_+$分别代表负例和正例的数目，如果两者接近，该阈值就为1。 查看更多所有的文章都会在我的博客和我的知乎专栏同步进行更新，欢迎阅读 我的博客 知乎专栏]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>logistics判别</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性回归和梯度下降]]></title>
    <url>%2F2018%2F07%2F05%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%2F</url>
    <content type="text"><![CDATA[在上一章我们说到，机器学习中主要的两个任务就是回归和分类。如果读者有高中数学基础，我们很容易回忆到我们高中学习过的一种回归方法——线性回归。我们将这种方法泛化，就可以得到机器学习中的一种常见模型——线性模型，线性模型是监督学习的一种。我们已经说过，我们要从数据集中训练出模型，每个数据可以视为（属性，标签）二元组。其中属性可以为属性向量。假设给定具有n个属性的属性向量的数据 $\vec x = (x_1,x_2,x_3 \dots x_n)$，我们利用属性的线性组合来进行预测，即$$f(x) = w_1x_1+w_2x_2+w_3x_3+ \dots + w_nx_n + b$$我们可以将其写成向量形式$$ f(x) = w^Tx+b$$其中$w=(w_1,w_2,w_3 \dots w_n)$，w和b就是该模型中我们要求的参数，确定w和b，该模型就得以确定。我们将这样的模型称为线性模型，不得不提的是，线性模型并不是只能进行线性分类，它具有很强的泛化能力，我们后面会提到。 属性转换在进行建模之前，我们要先对数据集进行处理，使得其适合进行建模。我们注意到，在线性模型中，属性值都是实数，那么会出现以下两种需要进行转化的情况 属性离散，但是有序关系（可以比较）。例如身材的过轻，正常，肥胖，过于肥胖，可以被编码为-1,0,1,2，从而转化为实数进行处理。 属性离散，但是无序关系（不可比较）。例如国籍的中国人，美国人，日本人。我们可以将取值有k种的值转化为k维向量，如上例，可以编码为 $(1,0,0),(0,1,0),(0,0,1)$。 单变量线性回归如果 $x = (x_1,x_2,x_3 \dots x_n)$中n= 1，此时x为一个实数，线性回归模型就退化为单变量线性回归。我们将模型记为$$f(x)=w x + b$$其中w,x,b都是实数,相信这个模型大家在高中都学习过。在这里我们有两种方法求解这个模型，分别是最小二乘法和梯度下降法。我们先定义符号，$x_i$ 代表第i个数据的属性值，$y_i$是第i个数据的标签值（即真值），f是我们学习到的模型，$f(x_i)$即我们对第i个数据的预测值。我们的目标是，求得适当的w和b，使得S最小，其中S是预测值和真值的差距平方和，亦称为代价函数，当然代价函数还有很多其他的形式。$$minS = \frac {1} {2n} \sum_{i=1}^{n} {(f(x_i)-y_i)^2}$$其中的$\frac1n$只是将代价函数值归一化的系数。 最小二乘法最小二乘法不是我们在这里要讨论的重点，但也是在很多地方会使用到的重要方法。最小二乘法使用参数估计，将S看做一个关于w和b的函数，分别对w和b求偏导数，使得偏导数为0，由微积分知识知道，在此次可以取得S的最小值。由这两个方程即可求得w和b的值。（此处省略过程）求得$$w = \frac {\sum_{i=1}^{n} {y_i(x_i-\overline x)}} {\sum_{i=1}^{n} {x_i^2} -\frac 1m(\sum_{i=1}^{n} {x_i})^2 }$$$$b = \overline y -b \overline x$$其中$\overline y，\overline x$分别是y和x的均值 梯度下降法我们刚刚利用了方程的方法求得了单变量线性回归的模型。但是对于几百万，上亿的数据，这种方法太慢了，这时，我们可以使用凸优化中最常见的方法之一——梯度下降法，来更加迅速的求得使得S最小的w和b的值。S可以看做w和b的函数 $S(w，b)$，这是一个双变量的函数，我们用matlab画出他的函数图像，可以看出这是一个明显的凸函数。梯度下降法的相当于我们下山的过程，每次我们要走一步下山，寻找最低的地方，那么最可靠的方法便是环顾四周，寻找能一步到达的最低点，持续该过程，最后得到的便是最低点。对于函数而言，便是求得该函数对所有参数（变量）的偏导，每次更新这些参数，直到到达最低点为止，注意这些参数必须在每一轮一起更新，而不是一个一个更新。过程如下：$$给w、b随机赋初值,一般可以都设为0$$$$w_{new} = w - a\frac {\partial S(w,b)}{\partial w}，b_{new} = b - a\frac {\partial S(w,b)}{\partial b}$$$$w = w_{new},b = b_{new}$$带入真正的表达式，即为$$w_0 = w_0 - a \frac1m \sum_{i=1}^{n} {(f(x_i)-y_i)},w_0是常数项$$$$w_j = w_j - a \frac1m \sum_{j=1}^{n} {(f(x_i)-y_i)x_i},j\in{1,2,3\cdots n}$$其中a为学习率，是一个实数。整个过程形象表示便是如下图所示，一步一步走，最后达到最低点。需要说明以下几点： a为学习率，学习率决定了学习的速度。 如果a过小，那么学习的时间就会很长，导致算法的低效，不如直接使用最小二乘法。 如果a过大，那么由于每一步更新过大，可能无法收敛到最低点。由于越偏离最低点函数的导数越大，如果a过大，某一次更新直接跨越了最低点，来到了比更新之前更高的地方。那么下一步更新步会更大，如此反复震荡，离最佳点越来越远。以上两种情况如下图所示 我们的算法不一定能达到最优解。如上图爬山模型可知，如果我们初始位置发生变化，那么可能会到达不同的极小值点。但是由于线性回归模型中的函数都是凸函数,所以利用梯度下降法，是可以找到全局最优解的，在这里不详细阐述。 多变量线性回归如果数据中属性是一个多维向量， $\vec x = (x_1,x_2,x_3 \dots x_n)$,那么该回归模型称为多变量线性回归。也就是一般意义上的线性回归模型。我们先定义符号，$\vec x_i$ 代表第i个数据的属性值，它是一个向量，$x_i^j$表示第i个数据的第j个属性，它是一个实数，$y_i$是第i个数据的标签值，也是实数。f是我们学习到的模型，$f(\vec x_i)$即我们对第i个数据的预测值。我们建立的模型为：$$f(\vec x_i) = \vec w^T \cdot \vec x + b$$我们的目标是，求得适当的 $\vec w$和b，使得S最小，其中S是预测值和真值的差距平方和，亦称为代价函数，当然代价函数还有很多其他的形式。$$minS = \frac {1} {2n} \sum_{i=1}^{n} {(f(\vec x_i)-y_i)^2}$$其中的$\frac1n$只是将代价函数值归一化的系数。 特征缩放由于$\vec x$具有很多维的特征，每一维的特征大小可能相差甚多，这样会大大影响学习的速度。假如房价范围0-10000000，房子大小范围1-200，那么这两个特征学习到的系数大小会差很多倍，而学习率必须按照最小的系数来进行设定，则大系数的收敛会非常慢。为了避免这种情况，我们使用了特征缩放将每个特征的值进行处理，使之在[-1,1]之间，当然，原本范围就于此在一个数量级的特征，也可以不进行处理。处理公式如下：$$x_i = \frac {x_i-\overline x} {x_{max}-x_{min}}$$或者$$x_i = \frac {x_i-\overline x} {\sigma}$$其中$\sigma$为数据标准差。 正规方程法对于多元线性回归而言，正规方程法是一种准确的方法，就像最小二乘法对于单变量线性回归一样。为了使形式更加简化，我们做以下符号设定$$\vec X = \left[ \begin{matrix} 1 &amp; x_1^1 &amp; x_1^2 &amp; x_1^3 \dots &amp; x_1^n \\ 1 &amp;x_2^1 &amp; x_2^2 &amp; x_2^3 \dots &amp; x_2^n \\ \cdots&amp; \cdots&amp; \cdots &amp; \cdots \\ 1 &amp;x_n^1 &amp; x_n^2 &amp; x_n^3 \dots &amp; x_n^n \\ \end{matrix} \right] \tag{3}$$由此，我们可以将S写成另一种形式，定义如下$$ S^1 = (\vec y -\vec X \cdot \vec w)^T(\vec y -\vec X \cdot \vec w) $$请注意，$S^1$和S的区别仅仅在于它没有$\frac 1n$的系数，而该系数是一个定值，故最小化的目标和过程是一样的，我们在此要将$S^1$最小化。同理，我们将$S^1$视为$\vec w$的函数，对于$\vec w$求导数，得到取得最小值时的$\vec w$的值，便是我们得到的结果，记为$\vec w^1$$$\vec w^1 =(\vec X^T \vec X)^{-1}\vec X^T\vec y$$该方法得到了为准确值，即在我们给定条件下的最优解，但是该方法有两个弊端： 需要计算$(\vec X^T \vec X)^{-1}$，相对于矩阵规模n而言，算法复杂度是O(n3), n非常大时, 计算非常慢，甚至根本无法完成。 可能出现矩阵不可逆的情况，在这里不进行数学上的分析，但是可以说明，以下两种情况容易导致矩阵不可逆。 我们使用了冗余的特征，例如我们选取的两个特征始终保持倍数关系，则这两个特征向量线性相关。此时应该去除冗余的向量。 我们使用了太多的特征(特征的数量超过了样本的数量).，也可以理解为样本的数量太少，对于这种情况我们可以删掉一些特征或者使用正则化（在下一篇文章中会讲到）。 梯度下降法此处的梯度下降法和之前一元线性回归的梯度下降法基本相同，无非是一元线性回归只有两个需要求的参数，而多元线性回归中有多个待求参数。其余的只需要将导数项换掉即可。最终得到的式子如下：$$w_0 = w_0 - a \frac1m \sum_{i=1}^{n} {(f(x_i)-y_i)},w_0是常数项$$$$w_j = w_j - a \frac1m \sum_{i=1}^{n} {(f(x_i)-y_i)x_i^j},j\in{1,2,3\cdots n}$$与正规方程法相比，梯度下降法当有大量特征时, 也能正常工作，仍可以在可接受的时间内完成。 泛化之前我们提到过，线性模型并不是只能进行线性分类，它具有很强的泛化能力，如果仅仅使用在此之前的单元和多元线性回归，我们只能得到多维空间的高维平面，为了进一步增强泛化能力，我们可以引入幂次项。比如我们原来有只有一个特征$x_1$，我们现在令$x_2=x_1^2$,就人为的引入了第二个特征，拥有更强的拟合能力。我们还可以引入两个特征的交叉项，使得线性模型更强大。例如，我们原本只有一个模型：$$y=w_1x_1+w_2x_2$$我们引入$x_3 = x_1^2,x_4=x_2^2,x_5=x_1x_2$，人为引入三个变量，我们的模型变为：$$y=w_1x_1+w_2x_2+w_3x_3+w_4x_4+w_5x_5$$也就是说，很多复杂的模型都可以转化为线性模型进行建模。但是，我们也要防范过拟合问题，过多的人为特征很容易导致过拟合，我们将在下一个章节详细讨论。 检验那么，我们写好算法进行运行之后，如何检验我们的算法是否正常运行呢？一个办法就是看他的S（总误差）随时间变化的图像。正常情况下，S应该随着算法的运行逐渐降低，降低的速度越来越小，但是如果算法错误，或者学习率不适宜，那么可能出现S反而增大或者抖动的现象，如下图所示： 总结线性模型以其简单和可解释性在众多模型中脱颖而出，至今仍是经常使用的回归算法之一，在机器学习中仍然具有重要应用，如趋势线，流行病学预测，金融经济等。 查看更多所有的文章都会在我的博客和我的知乎专栏同步进行更新，欢迎阅读 我的博客 知乎专栏]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习的概念、历史和未来]]></title>
    <url>%2F2018%2F06%2F29%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%A6%82%E5%BF%B5%E3%80%81%E5%8E%86%E5%8F%B2%E5%92%8C%E6%9C%AA%E6%9D%A5%2F</url>
    <content type="text"><![CDATA[相关概念提起机器学习，我们不得不给机器学习下一个准确的定义。在直观的层面，如果说计算机科学是研究关于算法的科学，那么机器学习就是研究关于“学习算法”的科学，或者说，不同于一般的显式编程，机器学习就是研究如何使得计算机在无法被显式编程的情况下进行学习的领域，需要注意的是，显式与否都是对于人类而言的——人类能否明确的搞清楚每个决策步骤，对于计算机而言，构成不同算法的代码与指令没有任何区别。更加精确的说，机器学习的定义如下： A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.一个（机器学习）的程序就是可以从经验数据E中对任务T进行学习的算法，它在任务T上的性能度量P会随着对于经验数据E的学习而变得更好 由于机器学习必然利用了某些经验，它们常常数据的形式存在，我们称之为数据集，其中的每个数据称为记录。例如我们通过一个人的性别、年龄和身高预测他是否患某种常见疾病，有以下数据： （性别：男；年龄：18；身高：174；是否得病：否）（性别：女；年龄：17；身高：164；是否得病：是）（性别：男；年龄：20；身高：181；是否得病：是）（性别：女；年龄：16；身高：161；是否得病：是） …… 这可以被称为一个数据集，其中每个人的数据称为记录。在记录中，关于该对象的描述型数据称为属性，由于属性往往有很多个——如上文的年龄，身高等，可以构成属性向量，这些向量张成的空间称为属性空间。而我们的算法需要预测那个量被称为标记（label）——在上文中便是“得病与否”。在有的数据集中存在标记，有的不存在。标记构成的空间称为标记空间，也称为输出空间。显然，由于我们只能得到整个总体数据的一部分——即训练样本，我们程序得到的模型却不能只适应于这个训练样本，它必须对整个总体数据都有比较好的预测效果。这就是说我们的模型必须具有泛化的能力。我们训练得到的模型称为一个假设，所有的模型一起构成了假设空间。显然，可能有多种假设空间和训练数据一致——就好像对于一个知识点很少的课堂学习，有不少人能得到很高的分数，但是对于整个总体数据，学习的不同模型显然效果差别很大——真正考验很多难的知识点的考试，考验把上述表面上的学霸分开。每个假设——也就是训练的模型，必然有其归纳偏好，也就是说，在训练集中没有见过的情况，或者两者皆可的情况，模型会选择哪种。归纳偏好是模型进行泛化的能力基础。那么，对于训练的得到多个不同模型，我们如何选择呢？常用的方法是奥卡姆剃刀： 奥卡姆剃刀：若有多个假设和观察一致，我们选择最简单的那个 奥卡姆剃刀基于一个朴素的哲学观念，即这个世界是简单的，可以理解的。 算法分类基于训练集是否拥有标记（label），我们可以把机器学习分为以下四类： 监督学习 无监督学习 半监督学习 强化学习 下面我们依次对他们进行解释。 监督学习：监督学习使用已知正确答案（label）的训练数据进行学习。就像一个学生得到了很多题目以及这些题目的答案，利用这些进行学习，最终希望可以做出更多的没有见过的题目。 无监督学习：无监督学习使用没有正确答案的数据进行学习。就像一个学生得到了很多练习题，尽管他不知道答案，但是他可以从中自行寻找规律。 半监督学习：该学习方法是以上两种的混合，在训练阶段结合了大量未标记的数据和少量标签数据。就好像一个学生得到了少量有答案的样例题目和大量无答案的练习题目进行学习。 强化学习：强化学习同样没有label，但是拥有回报函数来判断你是否更加接近目标。例如让学生搜寻某个正确答案，学生靠近正确答案，就进行奖励——比如给一个棒棒糖，如果更加偏离答案，就被杨永信电击一下，久而久之，学生会越来越靠近正确答案。 监督学习的任务亦可以分为两类： 分类：我们的目标应该是要对数据进行分类.，也就是说，我们预测的数据是离散的。例如：现在我们的数据是有关乳腺癌的医学数据, 它包含了肿瘤的大小以及该肿瘤是良性的还是恶性的. 我们的目标是给定一个肿瘤的大小来预测它是良性还是恶性. 回归：如果我们预测的数据是连续的，可以称为回归。例如： 我们想通过给定的一个房子的面积来预测这个房子在市场中的价格。 发展历程在历史上，人工智能的热潮和低谷已经度过了一轮又一轮，所以不得不提醒广大读者：一个技术必然是有其周期性，当前火热的深度学习完成不了强人工智能的历史使命，人工智能领域必然会再一次走向低谷，等待下一次技术迭代。那么机器学习和人工智能有什么关系呢？可以说，机器学习是人工智能发展到一定阶段的必然产物！从人们对于人工智能的认识来看，人工智能走过了以下几个阶段： 推理期：人们认为只要赋予机器以推理能力，机器就可以得到智能。典型代表：“逻辑理论家”程序，它证明了各种数学定理。 知识期：人们认为为了使机器有智能，不仅需要有逻辑推理能力，还需要有大量的知识。典型代表如专家系统。但人们发现需要输入的知识太多了，如果机器能自己学习知识，岂不是美滋滋？于是展开了关于机器学习的研究。 机器学习期：人们致力于研究如何让机器自己从样例中学习知识。 而对于机器学习而言，已经发展处以下一些流派，他们都在历史上繁荣一时，占据过一定的地位。 基于神经网络的“连接主义”：五十年代中后期闪亮登场，代表工作如”感知机“。 基于逻辑表达的“”符号主义“：六七十年代辉煌一时，代表工作如”结构学习系统“。 “统计学习”：九十年代中期开始兴起，代表技术是支持向量机和核方法。 目前，以深度学习为名的连接主义卷土而来，究其原因，不过是数据大了，计算能力强了——若数据样本过少，则容易“过拟合”，若没有强力计算设备，根本无法求解。所以，我在这里不得不再次提醒读者，目前深度学习并没有理论上的实质性突破，完成不了强人工智能的历史使命，人工智能领域必然会再一次走向低谷，等待下一次技术迭代，请不要把鸡蛋放在一个篮子里。 应用场景目前机器学习在各个领域发挥着重要领域，创造了无数的经济价值。以下举例说明 Google News搜集网上的新闻，并且根据新闻的主题将新闻分成许多簇, 然后将在同一个簇的新闻放在一起。 自动驾驶 结语机器学习拥有着广阔的应用场景和无限的前途，可以说，发展出能够取代人类的强人工智能，是整个计算机行业最大的目标。让我们一起交流学习，征服机器学习的星辰大海！ 查看更多所有的文章都会在我的博客和我的知乎专栏同步进行更新 我的博客 知乎专栏]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>概念</tag>
        <tag>历史</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习序言]]></title>
    <url>%2F2018%2F06%2F27%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%8F%E8%A8%80%2F</url>
    <content type="text"><![CDATA[专栏介绍机器学习专栏是我在2018年暑假开始全面入门机器学习的心得和总结，在这个为期接近三个月的暑假中，我会学习传统机器学习，深度学习，以及强化学习三类主要的机器学习方法。在本栏目中，我不会照葫芦画瓢的搬抄其他地方的资料，所有的文章均为原创，均为我的心得体会和总结。所以存在一定程度的简洁和省略。本专栏的目的是和想入门机器学习的朋友们一起交流，共同成长，也是对自己学习的一种监督和升华。本专栏所有文章都采用markdown书写，为保持风格一致，若有意为专栏投稿，请采取相同的格式。 作者介绍北大信科小码农一只，目前大二在读，爱好科幻、编程、游泳与思考，水平有限，梦想不小，愿与人交流，共同进步。个人知乎主页 学习资料 吴恩达机器学习课程-网易云课程 吴恩达深度学习-网易云课程 Reinforcement Learning 机器学习实战 Tensorflow：实战Google深度学习框架 TensorFlow实战 北大课程——游戏中的AI 西瓜书 更新事宜我会在学完并且理解一个章节后尽快进行更新，但不保障时间，更新时间尽量控制在一周两次左右。所有的文章都会在我的博客和我的知乎专栏同步进行更新 我的博客 知乎专栏 路漫漫其修远兮，吾将上下而求索，欢迎大家一起交流，共同学习]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>问候</tag>
        <tag>序言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你好，博客世界]]></title>
    <url>%2F2018%2F06%2F27%2F%E4%BD%A0%E5%A5%BD%EF%BC%8C%E5%8D%9A%E5%AE%A2%E4%B8%96%E7%95%8C%2F</url>
    <content type="text"><![CDATA[这是我的第一个博客，以后我会在这里分享知识、经验与看法~~~]]></content>
      <categories>
        <category>问候</category>
      </categories>
      <tags>
        <tag>测试</tag>
      </tags>
  </entry>
</search>
