<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>陈浩然的博客</title>
  
  <subtitle>计算机与经济的小屋&lt;br&gt;陈浩然♡张芷若</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://braveryCHR.github.io/"/>
  <updated>2018-07-16T08:26:39.129Z</updated>
  <id>https://braveryCHR.github.io/</id>
  
  <author>
    <name>bravery</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>logistics判别与线性模型中的问题</title>
    <link href="https://braveryCHR.github.io/2018/07/16/logistics%E5%88%A4%E5%88%AB%E4%B8%8E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>https://braveryCHR.github.io/2018/07/16/logistics判别与线性模型中的问题/</id>
    <published>2018-07-16T07:56:08.000Z</published>
    <updated>2018-07-16T08:26:39.129Z</updated>
    
    <content type="html"><![CDATA[<p>之前说过，机器学习的两大任务是<strong>回归</strong>和<strong>分类</strong>，上章的线性回归模型适合进行回归分析，例如预测房价，但是当输出的结果为离散值时，线性回归模型就不适用了。我们的任务是：<strong>将回归分析中的实数值转化为离散值或者对于离散值的概率</strong></p><h3 id="logistic判别"><a href="#logistic判别" class="headerlink" title="logistic判别"></a>logistic判别</h3><h4 id="转换函数"><a href="#转换函数" class="headerlink" title="转换函数"></a>转换函数</h4><p>例如我们进行<strong>癌症判定</strong>，回归模型可以输出癌症几率a，并且 $a\in (0,1)$，而我们的任务是将几率转化为0、1两个结果（例如0表示无癌，1表示患癌）。<br>如果我们使用前一章的线性回归模型，可以认为&gt;0.5的结果看成1，&lt;0.5的结果看成0，便可以得到下列的转换函数：<br>$$<br>result=0, y&lt;(=)0.5  \\<br>result=1, y&gt;(=)0.5 \\<br>$$<br>这个公式看上去十分直观，但是有一些明显的缺点：</p><ol><li>并非所有的连续值都可以均匀映射在[0,1]之间，例如人的身高在区间[120,250]之间，但是大部分人身高都取在该区间的<strong>中值</strong>，两端较少。</li><li>一些极端值可能会大大影响分类的效果，例如出现了一个身高0.7m的侏儒病患者，则映射区间变成了[70,250]-&gt;[0,1]，原来的中值180可能在这种情况下只能映射为0.3，使得分类效果变差。如下图所示： <img src="/image/2018-7-15-1.jpg" alt="参考图片"></li></ol><p>为了解决以上这些问题，我们提出了一个特殊的转换函数，<strong>对数几率函数</strong>，又称<strong>sigmoid函数</strong><br>$$<br>y=\frac {1} {1+e^{-x}}<br>$$<br>其图像如下： <img src="/image/2018-7-15-2.jpg" alt="参考图片"><br>可以很明显的看出，该函数将<strong>实数域映射成了[0,1]的区间</strong>，带入我们的线性回归方程，可得：<br>$$<br>y = \frac 1 {1+e^{-(\vec w^T \vec x+b)}}<br>$$<br>于是，无论线性回归取何值，我们都可以将其转化为[0,1]之间的值，经过变换可知：<br>$$<br>ln(\frac y {1-y}) = \vec w^T \vec x+b<br>$$<br>故在该函数中，$ln(\frac y {1-y})$ 代表了$\vec x$ 为正例的可行性大小，由于含有对数，所以称为对数几率，其中y为正例几率，1-y为反例几率。<br>所以在算法中，最终得到的结果y便代表是正例的几率，它在[0,1]之间，一般而言，<strong>如果y大于0.5，我们将其视为正例，如果y小于0.5，我们将其视为反例</strong>。</p><h4 id="更新策略"><a href="#更新策略" class="headerlink" title="更新策略"></a>更新策略</h4><p>我们将转换函数 $y = \frac 1 {1+e^{-(\vec w^T \vec x+b)}}$ 称为$h_w(x)$，这是原本的线性回归$ f(x) = w^Tx+b$ 和sigmoid函数$y=\frac {1} {1+e^{-x}}$结合得到的。<br>由于这个函数并不是凸函数，<strong>直接带入我们之前的梯度下降策略是无效的，得不到优化的结果</strong>，所以要更换梯度下降策略。<br>定义新的代价函数：<br>$$<br>Cost(h_w(x),y) = -log(h_w(x)) , y = 1 \\<br>Cost(h_w(x),y) = -log(1-h_w(x)) , y = 0\\<br>$$<br>观察该式可知，无论y取0还是1，当$h_w(x)$与y差距越大，代价函数值越大，且趋于正无穷，代价函数取值范围为$[0,+ \infty]$。<br>将上面的代价函数写成另一种形式，便于进行求导：<br>$$<br>Cost(h_w(x),y) = -ylog(h_w(x)) -(1-y)log(1-h_w(x))<br>$$<br>将所有的数据项代价累计起来，得到最终的代价函数形式：<br>$$<br>J(h_w(x),y) = -\frac1m \lbrace \sum_{i=1}^{m} y^ilog(h_w(x^i)) +(1-y^i)log(1-h_w(x^i)) \rbrace<br>$$<br>对于该函数使用<strong>梯度下降</strong>，分别对每一个$\vec w$的每一项进行求导和更新即可</p><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>当我们利用线性回归拟合数据时，为了拟合较为复杂的数据，可能会引入较多的参数，例如：<br>$$<br>y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \theta_4 x_4 \\<br>其中 x_1 = x , x_2 =x^2,x_3=x^3,x_4 = x^4<br>$$<br>我们可能会得到下面两种图像：<img src="/image/2018-7-16-1.jpg" alt="参考图片"><br>在理想情况下，我们的算法应该得到左边的图像，而右边的图像显然有<strong>过拟合</strong>的倾向。</p><blockquote><p>在统计学中，过拟合（英语：overfitting，或称过度拟合）现象是指在拟合一个统计模型时，使用过多参数。对比于可获取的数据总量来说，一个荒谬的模型只要足够复杂，是可以完美地适应数据。过拟合一般可以视为违反奥卡姆剃刀原则。当可选择的参数的自由度超过数据所包含信息内容时，这会导致最后（拟合后）模型使用任意的参数，这会减少或破坏模型一般化的能力更甚于适应数据。过拟合的可能性不只取决于参数个数和数据，也跟模型架构与数据的一致性有关。此外对比于数据中预期的噪声或错误数量，跟模型错误的数量也有关。</p></blockquote><p><strong>如果算法效果较好，即使我们在初始部分选择了很多个参数，如上文的$\theta_0 、 \theta_1 、 \theta_2 、\theta_3 、\theta_4$,理想的算法也应该尽量使$ \theta_3 、\theta_4$接近0</strong>，使得实际的得到的拟合曲线应该如左图所示，为解决该问题，我们引入了<strong>正则化</strong>项。</p><h4 id="正则化线性回归"><a href="#正则化线性回归" class="headerlink" title="正则化线性回归"></a>正则化线性回归</h4><p>为了解决过拟合的问题，我们应该引入一个参数项，使得<strong>在进行梯度下降的时候尽可能使得参数变小</strong>，这样可以使得很多额外的变量的系数接近于0。<br>更新线性回归的代价函数：<br>$$minS = \frac {1} {2n} (\sum_{i=1}^{n} {(f(x_i)-y_i)^2}+\lambda \sum_{j=1}^{m} \theta_j^2)$$<br>其中, $\lambda \sum_{j=1}^{m} \theta_j^2$叫做正则化项(Regularization Term), ,λ叫做正则化参数(Regularization Parameter). λ的作用就是在”更好地拟合数据”和”防止过拟合”之间权衡.λ过大的话, 就会导致$\theta_1 \theta_1$…近似于0, 这时就变成了<strong>欠拟合(Underfit)</strong>. 所以需要选择一个合适的λ。</p><h4 id="正则化logistics判别"><a href="#正则化logistics判别" class="headerlink" title="正则化logistics判别"></a>正则化logistics判别</h4><p>和正则化线性回归相似，我们也在logistics判别中加入正则化项：<br>$$<br>J(h_w(x),y) = -\frac1m \lbrace \sum_{i=1}^{m} y^ilog(h_w(x^i)) +(1-y^i)log(1-h_w(x^i)) \rbrace + \frac \lambda {2m} \sum_{j=1}^{m} \theta_j^2<br>$$<br>接着进行梯度下降即可。</p><h3 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h3><p>logistics判别解决的是二分类问题，那么应该如何解决多分类问题呢？一般采用拆解法，来将多分类问题分解成多个二分类问题。<br>一般而言有三种经典的拆分方法</p><ul><li><strong>一对一</strong>：假如某个分类中有N个类别，我们将这N个类别进行两两配对（两两配对后转化为二分类问题）。那么我们可以得到$\frac {N(N+1)} 2$个二分类器。之后在测试阶段，我们把新样本交给这个二分类器。于是我们可以得到个分类结果。把预测的最多的类别作为预测的结果。</li><li><strong>一对其余</strong>：一对其余其实更加好理解，每次将一个类别作为正类，其余类别作为负类。此时共有（N个分类器）。在测试的时候若仅有一个分类器预测为正类，则对应的类别标记为最终的分类结果。若有多个分类器预测为正类，则选择概率最大的那个。</li><li><strong>多对多</strong>：所谓多对多其实就是把多个类别作为正类，多个类别作为负类。该种方法比较复杂，这里推荐一个常用的方法：<a href="https://www.deeplearn.me/587.html" target="_blank" rel="noopener">ECOC纠错码方法</a>，这种方法简而言之，就是N个类别做多次划分形成M个分类器，用这M个分类器分别对N个类别进行识别，正例为1，负例为0，形成了<strong>M位的01编码</strong>，对于任何测试数据，形成其01编码，分别和N个类别的01编码进行比较，计算编码的距离，选取最近的类别作为测试数据的类别。</li></ul><h3 id="类别不均衡问题"><a href="#类别不均衡问题" class="headerlink" title="类别不均衡问题"></a>类别不均衡问题</h3><p>想象我们在做一个预测罕见病A的机器学习模型，但是该病十分罕见，我们一万个数据中只有8个病例，那么模型只需要将所有的数据都预测为无病，即可达到99.92%的超高预测成功率，但是显然这个模型不符合要求。<br>那么对于这种数据集中类别不平衡的问题，该如何解决呢？目前主要有三种方法：</p><ul><li><strong>欠采样</strong>：去除一些数目过多的类别的数据，使得不同类别的数据数目接近。<ul><li>优点：不需要重新收集数据，训练速度快</li><li>缺点：使用的数据集远小于原数据集，可能丢失重要信息</li></ul></li><li><strong>过采样</strong>：增加数目小的类别的数据，使得不同类别的数据数目接近。<ul><li>优点：不丢失信息，数据集较大</li><li>缺点：若对数目少的数据进行重复采样会造成过拟合的问题，训练时间长</li></ul></li><li><strong>阈值移动</strong>：我们在之前logistics判别中说过，$ln(\frac y {1-y}) = \vec w^T \vec x+b$，我们通过$\frac y {1-y}$的值来判断正例负例，之前我们的判断依据是 $if \frac y {1-y} &gt; 1  $ ，阈值为1，我们设定新的阈值$$\frac {y_1} {1-y_1} = \frac y {1-y} * \frac {m_-} {m_+}$$其中$m_-,m_+$分别代表负例和正例的数目，如果两者接近，该阈值就为1。</li></ul><h3 id="查看更多"><a href="#查看更多" class="headerlink" title="查看更多"></a>查看更多</h3><p>所有的文章都会在我的博客和我的知乎专栏同步进行更新，欢迎阅读</p><ul><li><a href="http://chrer.com/" target="_blank" rel="noopener">我的博客</a></li><li><a href="https://zhuanlan.zhihu.com/MLstudy" target="_blank" rel="noopener">知乎专栏</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;之前说过，机器学习的两大任务是&lt;strong&gt;回归&lt;/strong&gt;和&lt;strong&gt;分类&lt;/strong&gt;，上章的线性回归模型适合进行回归分析，例如预测房价，但是当输出的结果为离散值时，线性回归模型就不适用了。我们的任务是：&lt;strong&gt;将回归分析中的实数值转化为离散值
      
    
    </summary>
    
      <category term="机器学习" scheme="https://braveryCHR.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://braveryCHR.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="logistics判别" scheme="https://braveryCHR.github.io/tags/logistics%E5%88%A4%E5%88%AB/"/>
    
      <category term="线性回归" scheme="https://braveryCHR.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>线性回归和梯度下降</title>
    <link href="https://braveryCHR.github.io/2018/07/05/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    <id>https://braveryCHR.github.io/2018/07/05/线性回归和梯度下降/</id>
    <published>2018-07-05T11:30:05.000Z</published>
    <updated>2018-07-05T13:03:33.877Z</updated>
    
    <content type="html"><![CDATA[<p>在上一章我们说到，机器学习中主要的两个任务就是回归和分类。如果读者有高中数学基础，我们很容易回忆到我们高中学习过的一种回归方法——<strong>线性回归</strong>。我们将这种方法泛化，就可以得到机器学习中的一种常见模型——<strong>线性模型</strong>，线性模型是<strong>监督学习的一种</strong>。<br>我们已经说过，我们要从数据集中训练出模型，每个数据可以视为<strong>（属性，标签）</strong>二元组。其中属性可以为属性向量。<br>假设给定具有n个属性的属性向量的数据 $\vec x = (x_1,x_2,x_3 \dots x_n)$，我们利用属性的线性组合来进行预测，即<br>$$f(x) = w_1x_1+w_2x_2+w_3x_3+ \dots + w_nx_n + b$$<br>我们可以将其写成向量形式<br>$$ f(x) = w^Tx+b$$<br>其中$w=(w_1,w_2,w_3 \dots w_n)$，w和b就是该模型中我们要求的参数，确定w和b，该模型就得以确定。<br>我们将这样的模型称为线性模型，不得不提的是，线性模型并不是只能进行线性分类，它具有很强的泛化能力，我们后面会提到。</p><h3 id="属性转换"><a href="#属性转换" class="headerlink" title="属性转换"></a>属性转换</h3><p>在进行建模之前，我们要先对数据集进行处理，使得其适合进行建模。<br>我们注意到，在线性模型中，属性值都是<strong>实数</strong>，那么会出现以下两种需要进行转化的情况</p><ul><li><strong>属性离散，但是有序关系（可以比较）</strong>。例如身材的过轻，正常，肥胖，过于肥胖，可以被编码为-1,0,1,2，从而转化为实数进行处理。</li><li><strong>属性离散，但是无序关系（不可比较）</strong>。例如国籍的中国人，美国人，日本人。我们可以将取值有k种的值转化为k维向量，如上例，可以编码为 $(1,0,0),(0,1,0),(0,0,1)$。</li></ul><h3 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title="单变量线性回归"></a>单变量线性回归</h3><p>如果 $x = (x_1,x_2,x_3 \dots x_n)$中n= 1，此时x为一个实数，线性回归模型就退化为单变量线性回归。我们将模型记为<br>$$f(x)=w x + b$$<br>其中w,x,b都是实数,相信这个模型大家在高中都学习过。在这里我们有两种方法求解这个模型，分别是<strong>最小二乘法</strong>和<strong>梯度下降法</strong>。<br>我们先定义符号，$x_i$ 代表第i个数据的属性值，$y_i$是第i个数据的标签值（即真值），f是我们学习到的模型，$f(x_i)$即我们对第i个数据的预测值。<br>我们的目标是，求得适当的w和b，使得S最小，其中S是预测值和真值的差距平方和，亦称为<strong>代价函数</strong>，当然代价函数还有很多其他的形式。<br>$$minS = \frac {1} {2n} \sum_{i=1}^{n} {(f(x_i)-y_i)^2}$$<br>其中的$\frac1n$只是将代价函数值归一化的系数。</p><h4 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h4><p>最小二乘法不是我们在这里要讨论的重点，但也是在很多地方会使用到的重要方法。<br>最小二乘法使用参数估计，将S看做一个关于w和b的函数，分别对w和b求偏导数，使得偏导数为0，由微积分知识知道，在此次可以取得S的最小值。由这两个方程即可求得w和b的值。（此处省略过程）<br>求得<br>$$w = \frac {\sum_{i=1}^{n} {y_i(x_i-\overline x)}} {\sum_{i=1}^{n} {x_i^2} -\frac 1m(\sum_{i=1}^{n} {x_i})^2 }$$<br>$$b = \overline y -b \overline x$$<br>其中$\overline y，\overline x$分别是y和x的均值</p><h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p>我们刚刚利用了方程的方法求得了单变量线性回归的模型。但是对于几百万，上亿的数据，这种方法太慢了，这时，我们可以使用<strong>凸优化</strong>中最常见的方法之一——<strong>梯度下降法</strong>，来更加迅速的求得使得S最小的w和b的值。<br>S可以看做w和b的函数 $S(w，b)$，这是一个双变量的函数，我们用matlab画出他的函数图像，可以看出这是一个明显的凸函数。<br><img src="/image/2018-7-5-1.jpg" alt="参考图片"><br>梯度下降法的相当于<strong>我们下山的过程</strong>，每次我们要走一步下山，寻找最低的地方，那么最可靠的方法便是环顾四周，寻找能一步到达的最低点，持续该过程，最后得到的便是最低点。<br>对于函数而言，便是求得该函数对所有参数（变量）的偏导，每次更新这些参数，直到到达最低点为止，注意这些参数必须在每一轮一起更新，而不是一个一个更新。<br>过程如下：<br>$$给w、b随机赋初值,一般可以都设为0$$<br>$$w_{new} = w - a\frac {\partial S(w,b)}{\partial w}，b_{new} = b - a\frac {\partial S(w,b)}{\partial b}$$<br>$$w = w_{new},b = b_{new}$$<br>带入真正的表达式，即为<br>$$w_0 = w_0 - a \frac1m  \sum_{i=1}^{n} {(f(x_i)-y_i)},w_0是常数项$$<br>$$w_j = w_j - a \frac1m  \sum_{j=1}^{n} {(f(x_i)-y_i)x_i},j\in{1,2,3\cdots n}$$<br>其中a为学习率，是一个实数。整个过程形象表示便是如下图所示，一步一步走，最后达到最低点。<br><img src="/image/2018-7-5-2.jpg" alt="参考图片"><br>需要说明以下几点：</p><ul><li>a为学习率，学习率决定了学习的速度。<ul><li>如果a过小，那么学习的时间就会很长，导致算法的低效，不如直接使用最小二乘法。</li><li>如果a过大，那么由于每一步更新过大，可能无法收敛到最低点。由于越偏离最低点函数的导数越大，如果a过大，某一次更新直接跨越了最低点，来到了比更新之前<strong>更高</strong>的地方。那么下一步更新步会更大，如此反复震荡，离最佳点越来越远。以上两种情况如下图所示<br><img src="/image/2018-7-5-3.jpg" alt="参考图片"></li></ul></li><li>我们的算法不一定能达到最优解。如上图爬山模型可知，如果我们初始位置发生变化，那么可能会到达不同的极小值点。但是由于线性回归模型中的函数都是<a href="https://baike.baidu.com/item/%E5%87%B8%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">凸函数</a>,所以利用梯度下降法，是可以找到全局最优解的，在这里不详细阐述。</li></ul><h3 id="多变量线性回归"><a href="#多变量线性回归" class="headerlink" title="多变量线性回归"></a>多变量线性回归</h3><p>如果数据中属性是一个多维向量， $\vec x = (x_1,x_2,x_3 \dots x_n)$,那么该回归模型称为多变量线性回归。也就是一般意义上的线性回归模型。<br>我们先定义符号，$\vec x_i$ 代表第i个数据的属性值，它是一个向量，$x_i^j$表示第i个数据的第j个属性，它是一个实数，$y_i$是第i个数据的标签值，也是实数。f是我们学习到的模型，$f(\vec x_i)$即我们对第i个数据的预测值。<br>我们建立的模型为：<br>$$f(\vec x_i) = \vec w^T \cdot \vec x + b$$<br>我们的目标是，求得适当的 $\vec w$和b，使得S最小，其中S是预测值和真值的差距平方和，亦称为<strong>代价函数</strong>，当然代价函数还有很多其他的形式。<br>$$minS = \frac {1} {2n} \sum_{i=1}^{n} {(f(\vec x_i)-y_i)^2}$$<br>其中的$\frac1n$只是将代价函数值归一化的系数。</p><h4 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h4><p>由于$\vec x$具有很多维的特征，每一维的特征大小可能相差甚多，这样会大大影响学习的速度。假如房价范围0-10000000，房子大小范围1-200，那么这两个特征学习到的系数大小会差很多倍，而学习率必须按照最小的系数来进行设定，则大系数的收敛会非常慢。<br>为了避免这种情况，我们使用了特征缩放将每个特征的值进行处理，使之在[-1,1]之间，当然，原本范围就于此在一个数量级的特征，也可以不进行处理。处理公式如下：<br>$$x_i = \frac {x_i-\overline x} {x_{max}-x_{min}}$$<br>或者<br>$$x_i = \frac {x_i-\overline x} {\sigma}$$<br>其中$\sigma$为数据标准差。</p><h4 id="正规方程法"><a href="#正规方程法" class="headerlink" title="正规方程法"></a>正规方程法</h4><p>对于多元线性回归而言，正规方程法是一种准确的方法，就像最小二乘法对于单变量线性回归一样。<br>为了使形式更加简化，我们做以下符号设定<br>$$<br>\vec X =<br> \left[<br> \begin{matrix}<br>  1 &amp; x_1^1 &amp; x_1^2 &amp; x_1^3 \dots &amp; x_1^n  \\<br>  1 &amp;x_2^1 &amp; x_2^2 &amp; x_2^3 \dots &amp; x_2^n  \\<br>  \cdots&amp; \cdots&amp; \cdots &amp; \cdots \\<br>  1 &amp;x_n^1 &amp; x_n^2 &amp; x_n^3 \dots &amp; x_n^n  \\<br>  \end{matrix}<br>  \right] \tag{3}<br>$$<br>由此，我们可以将S写成另一种形式，定义如下<br>$$ S^1 = (\vec y -\vec X \cdot \vec w)^T(\vec y -\vec X \cdot \vec w) $$<br>请注意，$S^1$和S的区别仅仅在于它没有$\frac 1n$的系数，而该系数是一个定值，故最小化的目标和过程是一样的，我们在此要将$S^1$最小化。<br>同理，我们将$S^1$视为$\vec w$的函数，对于$\vec w$求导数，得到取得最小值时的$\vec w$的值，便是我们得到的结果，记为$\vec w^1$<br>$$\vec w^1 =(\vec X^T \vec X)^{-1}\vec X^T\vec y$$<br>该方法得到了为准确值，即在我们给定条件下的最优解，但是该方法有两个弊端：</p><ul><li>需要计算$(\vec X^T \vec X)^{-1}$，相对于矩阵规模n而言，算法复杂度是O(n3), n非常大时, 计算非常慢，甚至根本无法完成。</li><li>可能出现矩阵不可逆的情况，在这里不进行数学上的分析，但是可以说明，以下两种情况容易导致矩阵不可逆。<ul><li>我们使用了冗余的特征，例如我们选取的两个特征始终保持倍数关系，则这两个特征向量线性相关。此时应该去除冗余的向量。</li><li>我们使用了太多的特征(特征的数量超过了样本的数量).，也可以理解为样本的数量太少，对于这种情况我们可以删掉一些特征或者使用<strong>正则化</strong>（在下一篇文章中会讲到）。</li></ul></li></ul><h4 id="梯度下降法-1"><a href="#梯度下降法-1" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p>此处的梯度下降法和之前一元线性回归的梯度下降法基本相同，无非是一元线性回归只有两个需要求的参数，而多元线性回归中有多个待求参数。其余的只需要将导数项换掉即可。最终得到的式子如下：<br>$$w_0 = w_0 - a \frac1m  \sum_{i=1}^{n} {(f(x_i)-y_i)},w_0是常数项$$<br>$$w_j = w_j - a \frac1m  \sum_{i=1}^{n} {(f(x_i)-y_i)x_i^j},j\in{1,2,3\cdots n}$$<br>与正规方程法相比，梯度下降法当有大量特征时, 也能正常工作，仍可以在可接受的时间内完成。</p><h3 id="泛化"><a href="#泛化" class="headerlink" title="泛化"></a>泛化</h3><p>之前我们提到过，线性模型并不是只能进行线性分类，它具有很强的泛化能力，如果仅仅使用在此之前的单元和多元线性回归，<strong>我们只能得到多维空间的高维平面</strong>，为了进一步增强泛化能力，我们可以引入<strong>幂次项</strong>。<br>比如我们原来有只有一个特征$x_1$，我们现在令$x_2=x_1^2$,就人为的引入了第二个特征，拥有更强的拟合能力。<br>我们还可以引入两个特征的交叉项，使得线性模型更强大。<br>例如，我们原本只有一个模型：<br>$$y=w_1x_1+w_2x_2$$<br>我们引入$x_3 = x_1^2,x_4=x_2^2,x_5=x_1x_2$，人为引入三个变量，我们的模型变为：<br>$$y=w_1x_1+w_2x_2+w_3x_3+w_4x_4+w_5x_5$$<br>也就是说，很多复杂的模型都可以转化为线性模型进行建模。<br>但是，我们也要防范<strong>过拟合</strong>问题，过多的人为特征很容易导致过拟合，我们将在下一个章节详细讨论。</p><h3 id="检验"><a href="#检验" class="headerlink" title="检验"></a>检验</h3><p>那么，我们写好算法进行运行之后，如何检验我们的算法是否正常运行呢？一个办法就是看他的S（总误差）随时间变化的图像。<br>正常情况下，S应该随着算法的运行逐渐降低，降低的速度越来越小，但是如果算法错误，或者学习率不适宜，那么可能出现S反而增大或者抖动的现象，如下图所示：<br><img src="/image/2018-7-5-4.jpg" alt="参考图片"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>线性模型以其简单和可解释性在众多模型中脱颖而出，至今仍是经常使用的回归算法之一，在机器学习中仍然具有重要应用，如趋势线，流行病学预测，金融经济等。</p><h3 id="查看更多"><a href="#查看更多" class="headerlink" title="查看更多"></a>查看更多</h3><p>所有的文章都会在我的博客和我的知乎专栏同步进行更新，欢迎阅读</p><ul><li><a href="https://braverychr.github.io/">我的博客</a></li><li><a href="https://zhuanlan.zhihu.com/MLstudy" target="_blank" rel="noopener">知乎专栏</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在上一章我们说到，机器学习中主要的两个任务就是回归和分类。如果读者有高中数学基础，我们很容易回忆到我们高中学习过的一种回归方法——&lt;strong&gt;线性回归&lt;/strong&gt;。我们将这种方法泛化，就可以得到机器学习中的一种常见模型——&lt;strong&gt;线性模型&lt;/strong&gt;
      
    
    </summary>
    
      <category term="机器学习" scheme="https://braveryCHR.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://braveryCHR.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="https://braveryCHR.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="梯度下降" scheme="https://braveryCHR.github.io/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
  </entry>
  
  <entry>
    <title>机器学习的概念、历史和未来</title>
    <link href="https://braveryCHR.github.io/2018/06/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%A6%82%E5%BF%B5%E3%80%81%E5%8E%86%E5%8F%B2%E5%92%8C%E6%9C%AA%E6%9D%A5/"/>
    <id>https://braveryCHR.github.io/2018/06/29/机器学习的概念、历史和未来/</id>
    <published>2018-06-29T15:18:21.000Z</published>
    <updated>2018-06-29T15:21:17.374Z</updated>
    
    <content type="html"><![CDATA[<h3 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h3><p>提起机器学习，我们不得不给机器学习下一个准确的定义。在直观的层面，<strong>如果说计算机科学是研究关于算法的科学，那么机器学习就是研究关于“学习算法”的科学</strong>，或者说，不同于一般的显式编程，<strong>机器学习就是研究如何使得计算机在无法被显式编程的情况下进行学习的领域</strong>，需要注意的是，显式与否都是对于人类而言的——人类能否明确的搞清楚每个决策步骤，对于计算机而言，构成不同算法的代码与指令没有任何区别。<br>更加精确的说，机器学习的定义如下：</p><blockquote><p> A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.<br>一个（机器学习）的程序就是可以从经验数据E中对任务T进行学习的算法，它在任务T上的性能度量P会随着对于经验数据E的学习而变得更好</p></blockquote><p>由于机器学习必然利用了某些经验，它们常常<strong>数据</strong>的形式存在，我们称之为<strong>数据集</strong>，其中的每个数据称为<strong>记录</strong>。例如我们通过一个人的性别、年龄和身高预测他是否患某种常见疾病，有以下数据：</p><blockquote><p>（性别：男；年龄：18；身高：174；是否得病：否）<br>（性别：女；年龄：17；身高：164；是否得病：是）<br>（性别：男；年龄：20；身高：181；是否得病：是）<br>（性别：女；年龄：16；身高：161；是否得病：是） ……</p></blockquote><p>这可以被称为一个数据集，其中每个人的数据称为记录。在记录中，关于该对象的描述型数据称为<strong>属性</strong>，由于属性往往有很多个——如上文的年龄，身高等，可以构成<strong>属性向量</strong>，这些向量张成的空间称为<strong>属性空间</strong>。而我们的算法需要预测那个量被称为<strong>标记（label）</strong>——在上文中便是“得病与否”。在有的数据集中存在标记，有的不存在。标记构成的空间称为<strong>标记空间</strong>，也称为<strong>输出空间</strong>。<br>显然，由于我们只能得到整个总体数据的一部分——即<strong>训练样本</strong>，我们程序得到的模型却不能只适应于这个训练样本，它必须对整个总体数据都有比较好的预测效果。这就是说我们的模型必须具有<strong>泛化</strong>的能力。<br>我们训练得到的模型称为一个<strong>假设</strong>，所有的模型一起构成了<strong>假设空间</strong>。显然，可能有多种假设空间和训练数据一致——<strong>就好像对于一个知识点很少的课堂学习，有不少人能得到很高的分数</strong>，但是对于整个总体数据，学习的不同模型显然效果差别很大——<strong>真正考验很多难的知识点的考试，考验把上述表面上的学霸分开</strong>。<br>每个假设——也就是训练的模型，必然有其<strong>归纳偏好</strong>，也就是说，在训练集中没有见过的情况，或者两者皆可的情况，模型会选择哪种。归纳偏好是模型进行<strong>泛化</strong>的能力基础。<br>那么，对于训练的得到多个不同模型，我们如何选择呢？常用的方法是<strong>奥卡姆剃刀</strong>：</p><blockquote><p>奥卡姆剃刀：若有多个假设和观察一致，我们选择最简单的那个</p></blockquote><p>奥卡姆剃刀基于一个朴素的哲学观念，即这个世界是简单的，可以理解的。</p><h3 id="算法分类"><a href="#算法分类" class="headerlink" title="算法分类"></a>算法分类</h3><p>基于训练集是否拥有标记（label），我们可以把机器学习分为以下四类：</p><ul><li>监督学习</li><li>无监督学习</li><li>半监督学习</li><li>强化学习</li></ul><p>下面我们依次对他们进行解释。</p><ol><li>监督学习：监督学习使用已知正确答案（label）的训练数据进行学习。就像一个学生得到了很多题目以及这些题目的答案，利用这些进行学习，最终希望可以做出更多的没有见过的题目。</li><li>无监督学习：无监督学习使用没有正确答案的数据进行学习。就像一个学生得到了很多练习题，尽管他不知道答案，但是他可以从中自行寻找规律。</li><li>半监督学习：该学习方法是以上两种的混合，在训练阶段结合了大量未标记的数据和少量标签数据。就好像一个学生得到了少量有答案的样例题目和大量无答案的练习题目进行学习。</li><li>强化学习：强化学习同样没有label，但是拥有<strong>回报函数</strong>来判断你是否更加接近目标。例如让学生搜寻某个正确答案，学生靠近正确答案，就进行奖励——比如给一个棒棒糖，如果更加偏离答案，就被杨永信电击一下，久而久之，学生会越来越靠近正确答案。</li></ol><p>监督学习的任务亦可以分为两类：</p><ul><li>分类：我们的目标应该是要对数据进行分类.，也就是说，我们预测的数据是<strong>离散</strong>的。例如：现在我们的数据是有关乳腺癌的医学数据, 它包含了肿瘤的大小以及该肿瘤是良性的还是恶性的. 我们的目标是给定一个肿瘤的大小来预测它是良性还是恶性.<br><img src="/image/2018-6-29-1.jpg" alt="参考图片"></li><li>回归：如果我们预测的数据是<strong>连续</strong>的，可以称为回归。例如： 我们想通过给定的一个房子的面积来预测这个房子在市场中的价格。<br><img src="/image/2018-6-29-1.jpg" alt="参考图片"></li></ul><h3 id="发展历程"><a href="#发展历程" class="headerlink" title="发展历程"></a>发展历程</h3><p>在历史上，人工智能的热潮和低谷已经度过了一轮又一轮，所以不得不提醒广大读者：<strong>一个技术必然是有其周期性，当前火热的深度学习完成不了强人工智能的历史使命，人工智能领域必然会再一次走向低谷，等待下一次技术迭代</strong>。<br>那么机器学习和人工智能有什么关系呢？可以说，<strong>机器学习是人工智能发展到一定阶段的必然产物！</strong><br>从人们对于人工智能的认识来看，人工智能走过了以下几个阶段：</p><ol><li>推理期：人们认为只要赋予机器以推理能力，机器就可以得到智能。典型代表：“逻辑理论家”程序，它证明了各种数学定理。</li><li>知识期：人们认为为了使机器有智能，不仅需要有逻辑推理能力，还需要有大量的知识。典型代表如专家系统。但人们发现需要输入的知识太多了，如果机器能自己学习知识，岂不是美滋滋？于是展开了关于机器学习的研究。</li><li>机器学习期：人们致力于研究如何让机器自己从样例中学习知识。</li></ol><p>而对于机器学习而言，已经发展处以下一些流派，他们都在历史上繁荣一时，占据过一定的地位。</p><ul><li>基于神经网络的“连接主义”：五十年代中后期闪亮登场，代表工作如”感知机“。</li><li>基于逻辑表达的“”符号主义“：六七十年代辉煌一时，代表工作如”结构学习系统“。</li><li>“统计学习”：九十年代中期开始兴起，代表技术是支持向量机和核方法。</li></ul><p>目前，以<strong>深度学习为名的连接主义</strong>卷土而来，究其原因，不过是<strong>数据大了，计算能力强了</strong>——若数据样本过少，则容易“过拟合”，若没有强力计算设备，根本无法求解。<br>所以，我在这里不得不再次提醒读者，<strong>目前深度学习并没有理论上的实质性突破，完成不了强人工智能的历史使命，人工智能领域必然会再一次走向低谷，等待下一次技术迭代，请不要把鸡蛋放在一个篮子里</strong>。</p><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>目前机器学习在各个领域发挥着重要领域，创造了无数的经济价值。以下举例说明</p><ul><li>Google News搜集网上的新闻，并且根据新闻的主题将新闻分成许多簇, 然后将在同一个簇的新闻放在一起。<br><img src="/image/2018-6-29-3.jpg" alt="参考图片"></li><li>自动驾驶<br><img src="/image/2018-6-29-4.jpg" alt="参考图片"></li></ul><h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>机器学习拥有着广阔的应用场景和无限的前途，可以说，发展出能够取代人类的强人工智能，是整个计算机行业最大的目标。让我们一起交流学习，征服机器学习的星辰大海！</p><h3 id="查看更多"><a href="#查看更多" class="headerlink" title="查看更多"></a>查看更多</h3><p>所有的文章都会在我的博客和我的知乎专栏同步进行更新</p><ul><li><a href="https://braverychr.github.io/">我的博客</a></li><li><a href="https://zhuanlan.zhihu.com/MLstudy" target="_blank" rel="noopener">知乎专栏</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;相关概念&quot;&gt;&lt;a href=&quot;#相关概念&quot; class=&quot;headerlink&quot; title=&quot;相关概念&quot;&gt;&lt;/a&gt;相关概念&lt;/h3&gt;&lt;p&gt;提起机器学习，我们不得不给机器学习下一个准确的定义。在直观的层面，&lt;strong&gt;如果说计算机科学是研究关于算法的科学，那么
      
    
    </summary>
    
      <category term="机器学习" scheme="https://braveryCHR.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://braveryCHR.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="概念" scheme="https://braveryCHR.github.io/tags/%E6%A6%82%E5%BF%B5/"/>
    
      <category term="历史" scheme="https://braveryCHR.github.io/tags/%E5%8E%86%E5%8F%B2/"/>
    
  </entry>
  
  <entry>
    <title>机器学习序言</title>
    <link href="https://braveryCHR.github.io/2018/06/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%8F%E8%A8%80/"/>
    <id>https://braveryCHR.github.io/2018/06/27/机器学习序言/</id>
    <published>2018-06-27T15:31:31.000Z</published>
    <updated>2018-06-27T15:48:59.535Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/image/2018-6-27-1.png" alt="标题图像"></p><h3 id="专栏介绍"><a href="#专栏介绍" class="headerlink" title="专栏介绍"></a>专栏介绍</h3><p><strong>机器学习</strong>专栏是我在2018年暑假开始全面入门机器学习的心得和总结，在这个为期接近三个月的暑假中，我会学习<strong>传统机器学习</strong>，<strong>深度学习</strong>，以及<strong>强化学习</strong>三类主要的机器学习方法。<br>在本栏目中，我不会照葫芦画瓢的搬抄其他地方的资料，所有的文章均为原创，均为我的心得体会和总结。所以存在<strong>一定程度的简洁和省略</strong>。<br>本专栏的目的是和想入门机器学习的朋友们一起交流，共同成长，也是对自己学习的一种监督和升华。<br>本专栏所有文章都采用<strong>markdown</strong>书写，为保持风格一致，若有意为专栏投稿，请采取相同的格式。</p><h3 id="作者介绍"><a href="#作者介绍" class="headerlink" title="作者介绍"></a>作者介绍</h3><p>北大信科小码农一只，目前大二在读，爱好科幻、编程、游泳与思考，水平有限，梦想不小，愿与人交流，共同进步。<br><a href="https://www.zhihu.com/people/96-48/activities" target="_blank" rel="noopener">个人知乎主页</a></p><h3 id="学习资料"><a href="#学习资料" class="headerlink" title="学习资料"></a>学习资料</h3><ul><li><a href="http://study.163.com/course/introduction/1004570029.htm" target="_blank" rel="noopener">吴恩达机器学习课程-网易云课程</a></li><li><a href="http://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank" rel="noopener">吴恩达深度学习-网易云课程</a></li><li><a href="https://book.douban.com/subject/2866455/" target="_blank" rel="noopener">Reinforcement Learning</a></li><li><a href="https://book.douban.com/subject/24703171/" target="_blank" rel="noopener">机器学习实战</a></li><li><a href="https://book.douban.com/subject/26976457/" target="_blank" rel="noopener">Tensorflow：实战Google深度学习框架</a></li><li><a href="https://book.douban.com/subject/26974266/" target="_blank" rel="noopener">TensorFlow实战</a></li><li>北大课程——游戏中的AI</li></ul><h3 id="更新事宜"><a href="#更新事宜" class="headerlink" title="更新事宜"></a>更新事宜</h3><p>我会在学完并且理解一个章节后<strong>尽快</strong>进行更新，但不保障时间，更新时间尽量控制在一周两次左右。<br>所有的文章都会在我的博客和我的知乎专栏同步进行更新</p><ul><li><a href="https://braverychr.github.io/">我的博客</a></li><li><a href="https://zhuanlan.zhihu.com/MLstudy" target="_blank" rel="noopener">知乎专栏</a></li></ul><h4 id="路漫漫其修远兮，吾将上下而求索，欢迎大家一起交流，共同学习"><a href="#路漫漫其修远兮，吾将上下而求索，欢迎大家一起交流，共同学习" class="headerlink" title="路漫漫其修远兮，吾将上下而求索，欢迎大家一起交流，共同学习"></a>路漫漫其修远兮，吾将上下而求索，欢迎大家一起交流，共同学习</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/image/2018-6-27-1.png&quot; alt=&quot;标题图像&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;专栏介绍&quot;&gt;&lt;a href=&quot;#专栏介绍&quot; class=&quot;headerlink&quot; title=&quot;专栏介绍&quot;&gt;&lt;/a&gt;专栏介绍&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;机
      
    
    </summary>
    
      <category term="机器学习" scheme="https://braveryCHR.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://braveryCHR.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="问候" scheme="https://braveryCHR.github.io/tags/%E9%97%AE%E5%80%99/"/>
    
      <category term="序言" scheme="https://braveryCHR.github.io/tags/%E5%BA%8F%E8%A8%80/"/>
    
  </entry>
  
  <entry>
    <title>你好，博客世界</title>
    <link href="https://braveryCHR.github.io/2018/06/27/%E4%BD%A0%E5%A5%BD%EF%BC%8C%E5%8D%9A%E5%AE%A2%E4%B8%96%E7%95%8C/"/>
    <id>https://braveryCHR.github.io/2018/06/27/你好，博客世界/</id>
    <published>2018-06-27T10:15:14.000Z</published>
    <updated>2018-07-06T05:46:41.534Z</updated>
    
    <content type="html"><![CDATA[<p>这是我的第一个博客，以后我会在这里分享知识、经验与看法~~~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这是我的第一个博客，以后我会在这里分享知识、经验与看法~~~&lt;/p&gt;

      
    
    </summary>
    
      <category term="问候" scheme="https://braveryCHR.github.io/categories/%E9%97%AE%E5%80%99/"/>
    
    
      <category term="测试" scheme="https://braveryCHR.github.io/tags/%E6%B5%8B%E8%AF%95/"/>
    
  </entry>
  
</feed>
